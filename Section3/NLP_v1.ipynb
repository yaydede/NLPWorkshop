{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP with Python - Basics**\n",
    "\n",
    "Dr. Aydede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) and Large Language Models (LLMs) are grounded in statistical methods that rely on numerical representations rather than directly using words. LLMs represent the latest advancement in NLP, pushing the boundaries of text processing and generation. Converting text into numbers is a key step in both fields, and one common technique is word embedding. However, the approach to word embeddings in LLMs differs significantly from traditional NLP methods. In this context, weâ€™ll focus on word embeddings specifically within traditional NLP.\n",
    "\n",
    "Creating word embeddings from raw text involves several steps, such as text preprocessing, tokenization, and the application of embedding algorithms. Below, we provide a detailed overview of these steps along with commonly used Python libraries for each stage.\n",
    "\n",
    "## 1. Text Preprocessing:\n",
    "\n",
    "Preprocessing is crucial to clean and normalize the text data. This step typically includes:\n",
    "\n",
    "- Lowercasing\n",
    "- Removing punctuation\n",
    "- Removing stop words\n",
    "- Lemmatization or stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/YigitAydede/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/YigitAydede/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/YigitAydede/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/YigitAydede/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens: ['sample', 'financial', 'report', 'text', 'number', 'punctuation', 'various', 'stop', 'word']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download NLTK data files if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Downloading the missing resource\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample text (replace with your financial report text)\n",
    "text = \"This is a sample financial report text with numbers, punctuations, and various stop words.\"\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Removing punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Removing stop words and lemmatization\n",
    "    return processed_tokens\n",
    "\n",
    "processed_tokens = preprocess_text(text)\n",
    "print(\"Processed Tokens:\", processed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines download necessary data files from the NLTK (Natural Language Toolkit) library:\n",
    "\n",
    "- `nltk.download('punkt')`: Downloads the Punkt tokenizer models, which are used for tokenizing text into sentences and words.\n",
    "- `nltk.download('stopwords')`: Downloads a list of common stop words in various languages. Stop words are words that are commonly filtered out in natural language processing tasks because they don't carry significant meaning (e.g., \"and\", \"the\", \"is\").\n",
    "- `nltk.download('wordnet')`: Downloads the WordNet lexical database, which is used for lemmatization, finding synonyms, and other lexical tasks.\n",
    "- `nltk.download('omw-1.4')`: Downloads the Open Multilingual WordNet package, which is needed for certain WordNet functions, particularly for handling multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines initialize the tools for lemmatization and stop word removal:\n",
    "\n",
    "- `WordNetLemmatizer()`: Creates an instance of the WordNet lemmatizer, which reduces words to their base or root form (e.g., \"running\" becomes \"run\").\n",
    "- `set(stopwords.words('english'))`: Creates a set of English stop words to efficiently check if a word is a stop word. Using a set makes membership tests faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuvUmUgE_IgE"
   },
   "source": [
    "## 2. `word2vec`\n",
    "\n",
    "Word embedding is a technique in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually, it involves the mathematical embedding from some space (e.g., the space of all possible words) to a lower-dimensional space of the real numbers. The key idea is to capture the semantic meanings, syntactic similarity, and relation of words in these vectors, such that words with similar meanings are closer to each other in the vector space.\n",
    "\n",
    "Word embedding models like Word2Vec, GloVe (Global Vectors for Word Representation), and FastText have become foundational in NLP applications because they can reduce the dimensionality of text data while preserving lexical and semantic word relationships.\n",
    "\n",
    "Let's take a simple example using Word2Vec from the Gensim library. Word2Vec can be trained with either the Continuous Bag of Words (CBOW) or Skip-Gram model. In CBOW, the model predicts a word given its context. In Skip-Gram, it predicts the context given a word. Here's how you can use Gensim to train a simple Word2Vec model on a small dataset:\n",
    "\n",
    "1. First, we'll create a small dataset (corpus).\n",
    "2. Then, we'll train a Word2Vec model on this dataset.\n",
    "3. Finally, we'll explore the resulting word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ik-e7US5cHyV",
    "outputId": "497b901c-b1ac-4aa0-f85a-f7e8f988bc5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 15:26:50,270 : INFO : collecting all words and their counts\n",
      "2024-08-25 15:26:50,271 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-08-25 15:26:50,272 : INFO : collected 18 word types from a corpus of 24 raw words and 4 sentences\n",
      "2024-08-25 15:26:50,272 : INFO : Creating a fresh vocabulary\n",
      "2024-08-25 15:26:50,273 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 18 unique words (100.00% of original 18, drops 0)', 'datetime': '2024-08-25T15:26:50.273001', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:26:50,273 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 24 word corpus (100.00% of original 24, drops 0)', 'datetime': '2024-08-25T15:26:50.273295', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:26:50,273 : INFO : deleting the raw counts dictionary of 18 items\n",
      "2024-08-25 15:26:50,273 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2024-08-25 15:26:50,274 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3.5679764105789453 word corpus (14.9%% of prior 24)', 'datetime': '2024-08-25T15:26:50.274284', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:26:50,274 : INFO : estimated required memory for 18 words and 100 dimensions: 23400 bytes\n",
      "2024-08-25 15:26:50,275 : INFO : resetting layer weights\n",
      "2024-08-25 15:26:50,275 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-08-25T15:26:50.275542', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-08-25 15:26:50,275 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 18 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-08-25T15:26:50.275787', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-25 15:26:50,277 : INFO : EPOCH 0: training on 24 raw words (4 effective words) took 0.0s, 8234 effective words/s\n",
      "2024-08-25 15:26:50,278 : INFO : EPOCH 1: training on 24 raw words (4 effective words) took 0.0s, 39523 effective words/s\n",
      "2024-08-25 15:26:50,279 : INFO : EPOCH 2: training on 24 raw words (3 effective words) took 0.0s, 8667 effective words/s\n",
      "2024-08-25 15:26:50,280 : INFO : EPOCH 3: training on 24 raw words (3 effective words) took 0.0s, 9641 effective words/s\n",
      "2024-08-25 15:26:50,281 : INFO : EPOCH 4: training on 24 raw words (3 effective words) took 0.0s, 11685 effective words/s\n",
      "2024-08-25 15:26:50,282 : INFO : Word2Vec lifecycle event {'msg': 'training on 120 raw words (17 effective words) took 0.0s, 2755 effective words/s', 'datetime': '2024-08-25T15:26:50.282147', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-25 15:26:50,282 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=18, vector_size=100, alpha=0.025>', 'datetime': '2024-08-25T15:26:50.282335', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model: Word2Vec<vocab=18, vector_size=100, alpha=0.025>\n",
      "Vector for 'python': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "Words similar to 'python': [('learning', 0.21883945167064667), ('with', 0.21617388725280762), ('easy', 0.0931011214852333), ('and', 0.0929069072008133), ('programs', 0.07961685955524445), ('languages', 0.06285078078508377), ('machine', 0.05433366447687149), ('java', 0.02702956274151802), ('language', 0.016139546409249306), ('is', -0.01083618775010109)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "# Enable logging for monitoring training\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['python', 'is', 'a', 'programming', 'language'],\n",
    "    ['python', 'and', 'java', 'are', 'popular', 'programming', 'languages'],\n",
    "    ['python', 'programs', 'are', 'easy', 'to', 'write'],\n",
    "    ['machine', 'learning', 'is', 'fun', 'with', 'python']\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Summarize the loaded model\n",
    "print(\"Word2Vec model:\", model)\n",
    "\n",
    "# Access vectors for one word\n",
    "print(\"Vector for 'python':\", model.wv['python'])\n",
    "\n",
    "# Find most similar words\n",
    "print(\"Words similar to 'python':\", model.wv.most_similar('python'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the basics of training a Word2Vec model with Gensim. Here, `vector_size` specifies the dimensionality of the word vectors, `window` defines the maximum distance between the current and predicted word within a sentence, and `min_count` ignores all words with total frequency lower than this.\n",
    "\n",
    "After training, we access the vector for \"python\" and find words similar to \"python\" based on their word embeddings. The output will give you an insight into how the model understands \"python\" in the context of the provided corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embeddings - Details\n",
    "\n",
    "### Popular Word Embedding Models\n",
    "These are some of the most popular and widely used word embedding models: `Word2Vec`, `GloVe`, and `FastText`.\n",
    "\n",
    "`Word2Vec`, `GloVe`, and `FastText` are different in their approaches to generating word embeddings, though they share the common goal of representing words as vectors in a continuous vector space. Hereâ€™s a brief comparison:\n",
    "\n",
    "1. `Word2Vec`\n",
    "- Developed by: Google.\n",
    "- Approach: Predictive model.\n",
    "- Architecture: Uses neural networks with either Continuous Bag of Words (CBOW) or Skip-gram models.\n",
    "    - CBOW: Predicts a target word from a window of context words.\n",
    "    - Skip-gram: Predicts context words from a target word.\n",
    "- Training: Trained on large corpora of text, learning to predict words given their context.\n",
    "- Output: Dense vector representations for each word.\n",
    "\n",
    "2. `GloVe`\n",
    "- Developed by: Stanford.\n",
    "- Approach: Count-based model.\n",
    "- Architecture: Constructs a co-occurrence matrix of words from a corpus, then factorizes this matrix to find word vectors.\n",
    "- Training: Uses the statistical information contained in a corpus, specifically the co-occurrence matrix, to find vector representations that capture the probability of word co-occurrences.\n",
    "- Output: Dense vector representations for each word.\n",
    "\n",
    "3. `FastText`\n",
    "- Developed by: Facebook.\n",
    "- Approach: Predictive model with subword information.\n",
    "- Architecture: Similar to Word2Vec (CBOW or Skip-gram), but includes subword (character n-grams) information.\n",
    "- Training: Trains on large corpora of text, learning to predict words given their context, while incorporating subword information.\n",
    "- Output: Dense vector representations for each word, incorporating subword information.\n",
    "\n",
    "Each method has its own strengths and weaknesses, and the choice between them depends on the specific requirements of the task at hand, such as the size of the training data, the importance of handling out-of-vocabulary words, and the computational resources available.\n",
    "\n",
    "### Word2Vec - CBOW & Skip-gram\n",
    "Word2Vec is a popular technique for generating word embeddings, which are dense vector representations of words in a continuous vector space. There are two main approaches within Word2Vec: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "\n",
    "#### Continuous Bag of Words (CBOW)\n",
    "CBOW predicts the current word based on the context words surrounding it. It uses a sliding window of fixed size to capture the context around the target word. The model learns to predict the target word given the context words.\n",
    "\n",
    "How CBOW works:\n",
    "1. Context Window: Select a context window size (e.g., 2 words on either side of the target word).\n",
    "2. Context Words: For a given target word, identify the words within the context window.\n",
    "3. Prediction: Use the context words as input to predict the target word.\n",
    "Learning: The neural network adjusts its weights to minimize the prediction error.\n",
    "\n",
    "Example:\n",
    "- Sentence: \"The quick brown fox jumps over the lazy dog\"\n",
    "- Context window size = 2\n",
    "- Target word = \"brown\"\n",
    "- Context words = [\"The\", \"quick\", \"fox\", \"jumps\"]\n",
    "\n",
    "The CBOW model would take the context words (\"The\", \"quick\", \"fox\", \"jumps\") as input and try to predict the target word \"brown\".\n",
    "\n",
    "#### Skip-gram\n",
    "Skip-gram, on the other hand, predicts the context words based on the current word. It uses the current word as the input and predicts the surrounding context words.\n",
    "\n",
    "How it works:\n",
    "1. Context Window: Select a context window size (e.g., 2 words on either side of the target word).\n",
    "2. Target Word: For a given context window, identify the target word.\n",
    "3. Prediction: Use the target word as input to predict the context words.\n",
    "Learning: The neural network adjusts its weights to minimize the prediction error.\n",
    "\n",
    "Example:\n",
    "- Sentence: \"The quick brown fox jumps over the lazy dog\"\n",
    "- Context window size = 2\n",
    "- Target word = \"brown\"\n",
    "- Context words = [\"The\", \"quick\", \"fox\", \"jumps\"]\n",
    "\n",
    "The Skip-gram model would take the target word \"brown\" as input and try to predict the context words (\"The\", \"quick\", \"fox\", \"jumps\").\n",
    "\n",
    "#### Comparison\n",
    "- CBOW: Faster to train, works well with smaller datasets, and averages the context, which can smooth out noise.\n",
    "- Skip-gram: Slower to train but can produce higher-quality embeddings, especially for infrequent words, as it focuses on each context-target pair individually.\n",
    "\n",
    "Both architectures aim to create word embeddings that capture semantic relationships between words based on their contexts, but they do so in different ways. CBOW predicts the target word from its context, while Skip-gram predicts the context from the target word.\n",
    "\n",
    "### Data structure\n",
    "\n",
    "To understand how the data looks in CBOW (Continuous Bag of Words) using `Word2Vec` with one-hot encoding, let's walk through an example step-by-step.  Suppose we have 5 sentences each has different numbers of words\n",
    "\n",
    "When dealing with multiple sentences of different lengths in the context of training `Word2Vec` using the CBOW model, the process involves creating context-target pairs for each sentence individually. Here's how it works:\n",
    "\n",
    "General Approach\n",
    "1. Tokenization: Each sentence is broken down into individual words.\n",
    "2. Vocabulary Creation: A vocabulary of unique words across all sentences is created. Each word is assigned a unique index.\n",
    "3. One-Hot Encoding: Each word in the vocabulary is represented as a one-hot encoded vector.\n",
    "\n",
    "The CBOW model processes each sentence independently, generating context-target pairs based on the chosen context window size. Let's illustrate this with an example:\n",
    "\n",
    "Example Sentences\n",
    "1. \"I love natural language processing.\"\n",
    "2. \"Word2Vec is a popular algorithm.\"\n",
    "3. \"CBOW and Skip-gram are two models.\"\n",
    "4. \"Training word embeddings is important.\"\n",
    "5. \"Handling different sentence lengths.\"\n",
    "\n",
    "First, we tokenize each sentence:\n",
    "\n",
    "1. [\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n",
    "2. [\"Word2Vec\", \"is\", \"a\", \"popular\", \"algorithm\"]\n",
    "3. [\"CBOW\", \"and\", \"Skip-gram\", \"are\", \"two\", \"models\"]\n",
    "4. [\"Training\", \"word\", \"embeddings\", \"is\", \"important\"]\n",
    "5. [\"Handling\", \"different\", \"sentence\", \"lengths\"]\n",
    "\n",
    "Then, we create a vocabulary (Assume each word is assigned an index based on its order in the vocabulary):\n",
    "\"I\",Â \"love\",Â \"natural\",Â \"language\",Â \"processing\",Â \"Word2Vec\",Â \"is\",Â \"a\",Â \"popular\",Â \"algorithm\",Â \"CBOW\",Â \"and\",Â \"Skip-gram\",Â \"are\",Â \"two\",Â \"models\",Â \"Training\",Â \"word\",Â \"embeddings\",Â \"important\",Â \"Handling\",Â \"different\",Â \"sentence\",Â \"lengths\"\n",
    "\n",
    "#### Context-Target Pairs\n",
    "\n",
    "Now, let's create context-target pairs for each sentence:\n",
    "\n",
    "For each sentence, context-target pairs are generated based on the chosen context window size. Let's assume the context window size is 2.\n",
    "\n",
    "Sentence 1: \"I love natural language processing.\"\n",
    "1. Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "2. Target: \"language\" | Context: [\"love\", \"natural\"]\n",
    "3. Target: \"processing\" | Context: [\"natural\", \"language\"]\n",
    "Sentence 2: \"Word2Vec is a popular algorithm.\"\n",
    "1. Target: \"is\" | Context: [\"Word2Vec\"]\n",
    "2. Target: \"a\" | Context: [\"Word2Vec\", \"is\"]\n",
    "3. Target: \"popular\" | Context: [\"is\", \"a\"]\n",
    "4. Target: \"algorithm\" | Context: [\"a\", \"popular\"]\n",
    "\n",
    "And so on.  In the case of sentences with different lengths, the process remains the same. The context-target pairs are generated for each sentence independently. The context window size is applied to each sentence, and the target word is selected from the context window.\n",
    "\n",
    "#### Understanding the Input Matrix for CBOW\n",
    "\n",
    "1. Context Window: For each target word, we consider a window of context words around it. Let's assume a context window size of 2 for this explanation.\n",
    "2. Context-Target Pairs: Each pair consists of context words as input and a target word as output.\n",
    "3. One-Hot Encoding: Each word in the context and target is represented as a one-hot encoded vector of length equal to the vocabulary size (24 in this case).\n",
    "\n",
    "Example Breakdown\n",
    "Given the five sentences and a vocabulary of 24 unique words, let's outline the context-target pairs and how they form the input matrix.\n",
    "\n",
    "Generating Context-Target Pairs for the first 2 sentences:\n",
    "Sentence 1: \"I love natural language processing.\"\n",
    "Pairs:\n",
    "1. Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "2. Target: \"language\" | Context: [\"love\", \"natural\"]\n",
    "3. Target: \"processing\" | Context: [\"natural\", \"language\"]\n",
    "Sentence 2: \"Word2Vec is a popular algorithm.\"\n",
    "Pairs:\n",
    "1. Target: \"is\" | Context: [\"Word2Vec\"]\n",
    "2. Target: \"a\" | Context: [\"Word2Vec\", \"is\"]\n",
    "3. Target: \"popular\" | Context: [\"is\", \"a\"]\n",
    "4. Target: \"algorithm\" | Context: [\"a\", \"popular\"]\n",
    "\n",
    "Context Matrix Structure\n",
    "For each context-target pair, the context words are one-hot encoded, and these one-hot encoded vectors are concatenated to form the input matrix.\n",
    "\n",
    "Example Pair\n",
    "Pair: Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "\n",
    "One-hot Encoding:  \n",
    "\"I\": [1, 0, 0, 0, ..., 0]\n",
    "\"love\": [0, 1, 0, 0, ..., 0]\n",
    "\n",
    "If we consider each context-target pair as an individual training example, the input (context) matrix for all pairs combined would be structured with each row representing a context word vector and columns representing features (words in the vocabulary).\n",
    "\n",
    "Example Scenario\n",
    "Given the same sentence: \"I love natural language processing.\"\n",
    "\n",
    "Vocabulary size (V) = 10\n",
    "Context window size = 2\n",
    "One-Hot Encoding Representation\n",
    "Assume the indices for one-hot encoding are:\n",
    "\n",
    "I -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "love -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "natural -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "language -> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "processing -> [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "Word2Vec -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "is -> [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "a -> [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "popular -> [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "algorithm -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "Context-Target Pairs for \"I love natural language processing.\"\n",
    "For context window size of 2 :\n",
    "1. Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "- Input (context): Sum of $[1,0,0,0,0,0,0,0,0,0]$ and $[0,1,0,0,0,0,0,0,0,0]$\n",
    "- Target: $[0,0,1,0,0,0,0,0,0,0]$\n",
    "2. Target: \"language\" | Context: [\"love\", \"natural\"]\n",
    "- Input (context): Sum of $[0,1,0,0,0,0,0,0,0,0]$ and $[0,0,1,0,0,0,0,0,0,0]$\n",
    "- Target: $[0,0,0,1,0,0,0,0,0,0]$\n",
    "3. Target: \"processing\" | Context: [\"natural\", \"language\"]\n",
    "- Input (context): Sum of $[0,0,1,0,0,0,0,0,0,0]$ and $[0,0,0,1,0,0,0,0,0,0]$\n",
    "- Target: $[0,0,0,0,1,0,0,0,0,0]$\n",
    "\n",
    "Input and Target Matrices\n",
    "Input Matrix (Summed Context Vectors):\n",
    "$$\n",
    "\\left[\\begin{array}{llllllllll}\n",
    "1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Target Matrix:\n",
    "$$\n",
    "\\left[\\begin{array}{llllllllll}\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "By summing the one-hot encoded vectors of the context words, you create a combined context representation that keeps the 1s from each context word. This approach simplifies the input matrix and ensures the dimensions are aligned correctly:\n",
    "\n",
    "Input Matrix Dimensions: Number of context-target pairs (3) Ã— Vocabulary size (10)\n",
    "Target Matrix Dimensions: Number of context-target pairs (3) Ã— Vocabulary size (10)\n",
    "This ensures that each row in the input matrix corresponds to a combined context vector, and each row in the target matrix corresponds to the target word's one-hot encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zviEL7BAh92"
   },
   "source": [
    "\n",
    "### **It's single layer NN with 100 nodes**\n",
    "  \n",
    "Let's clarify a bit more about what happens inside Word2Vec.\n",
    "\n",
    "Word2Vec, whether using the Continuous Bag of Words (CBOW) or Skip-Gram model, indeed leverages a neural network architecture, but it's structured a bit differently than a typical feedforward neural network with a single layer of 100 nodes (when you set vector_size=100). The \"100 nodes\" or \"100 dimensions\" represent the size of the word vectors you're aiming to learn, not the nodes of a hidden layer in a traditional sense.\n",
    "\n",
    "Here's a simplified overview of the process for both CBOW and Skip-Gram models:\n",
    "\n",
    "Input Layer: For CBOW, the input is the context words (multiple words), which are one-hot encoded vectors representing the presence of words in the context. For Skip-Gram, the input is just the target word. The size of each input vector is equal to the vocabulary size.\n",
    "\n",
    "Projection Layer (or Hidden Layer): This is not a typical hidden layer with activation functions. Instead, it's a projection layer where the actual learning of word embeddings happens. When you set vector_size=100, it means this layer will have 100 neurons. The weights connecting the input layer to this layer are what become the word embeddings. In training, for a given input word, the corresponding row in the weight matrix is essentially the word vector for that word.\n",
    "\n",
    "In CBOW, the vectors from the projection layer corresponding to each context word are averaged before being passed to the output layer.\n",
    "In Skip-Gram, the projection layer directly connects to the output layer, using the vector of the input word.\n",
    "Output Layer: The output layer is a softmax layer that makes predictions. For CBOW, it predicts the target word from the context. For Skip-Gram, it predicts the context words from the target word. The size of this layer is also equal to the vocabulary size.\n",
    "\n",
    "So, in summary:\n",
    "\n",
    "The \"100 dimensions\" are essentially the weights of the projection layer that you learn during training.\n",
    "  \n",
    "The learning involves adjusting these weights so that the model gets better at its prediction task (predicting context words for Skip-Gram, predicting a target word for CBOW), thereby capturing semantic and syntactic word relationships in the process.\n",
    "The neural network aspect of Word2Vec is quite specialized and optimized for the task of learning word embeddings, which is a bit different from a general-purpose neural network used for other types of prediction tasks.\n",
    "\n",
    "In the context of the Word2Vec architecture and specifically regarding the projection (or hidden) layer where the word embeddings are learned, the activation function can indeed be thought of as an identity function, $f(x)=x$. This means that the output of each neuron in this layer is the same as its input, without any nonlinear transformation applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOXS5MrTdf8G",
    "outputId": "5ad31e42-d235-485c-803b-05bc57c7221d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/YigitAydede/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "comments = [\"love the new features\", \"hate the long wait time\", \"excellent service\", \"poor experience with the product\", \"happy with the purchase\"]\n",
    "sentiments = [1, 0, 1, 0, 1]  # 1: Positive, 0: Negative\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Now you can proceed with tokenizing your text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69oCTJ0fgtS9",
    "outputId": "8fe50d22-35f1-43d8-f6a9-dac1ad1f5d69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 15:27:09,649 : INFO : collecting all words and their counts\n",
      "2024-08-25 15:27:09,650 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-08-25 15:27:09,650 : INFO : collected 16 word types from a corpus of 20 raw words and 5 sentences\n",
      "2024-08-25 15:27:09,651 : INFO : Creating a fresh vocabulary\n",
      "2024-08-25 15:27:09,651 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 16 unique words (100.00% of original 16, drops 0)', 'datetime': '2024-08-25T15:27:09.651826', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:27:09,652 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 20 word corpus (100.00% of original 20, drops 0)', 'datetime': '2024-08-25T15:27:09.652102', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:27:09,652 : INFO : deleting the raw counts dictionary of 16 items\n",
      "2024-08-25 15:27:09,652 : INFO : sample=0.001 downsamples 16 most-common words\n",
      "2024-08-25 15:27:09,652 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2.7827416997969525 word corpus (13.9%% of prior 20)', 'datetime': '2024-08-25T15:27:09.652882', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:27:09,653 : INFO : estimated required memory for 16 words and 4 dimensions: 8512 bytes\n",
      "2024-08-25 15:27:09,653 : INFO : resetting layer weights\n",
      "2024-08-25 15:27:09,653 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-08-25T15:27:09.653888', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-08-25 15:27:09,654 : INFO : Word2Vec lifecycle event {'msg': 'training model with 1 workers on 16 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-08-25T15:27:09.654149', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-25 15:27:09,655 : INFO : EPOCH 0: training on 20 raw words (3 effective words) took 0.0s, 11401 effective words/s\n",
      "2024-08-25 15:27:09,655 : INFO : EPOCH 1: training on 20 raw words (4 effective words) took 0.0s, 11791 effective words/s\n",
      "2024-08-25 15:27:09,656 : INFO : EPOCH 2: training on 20 raw words (1 effective words) took 0.0s, 63327 effective words/s\n",
      "2024-08-25 15:27:09,657 : INFO : EPOCH 3: training on 20 raw words (2 effective words) took 0.0s, 9826 effective words/s\n",
      "2024-08-25 15:27:09,657 : INFO : EPOCH 4: training on 20 raw words (4 effective words) took 0.0s, 254631 effective words/s\n",
      "2024-08-25 15:27:09,658 : INFO : Word2Vec lifecycle event {'msg': 'training on 100 raw words (14 effective words) took 0.0s, 3836 effective words/s', 'datetime': '2024-08-25T15:27:09.658061', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-25 15:27:09,658 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16, vector_size=4, alpha=0.025>', 'datetime': '2024-08-25T15:27:09.658273', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'love': [-0.03944132  0.00803429 -0.10351574 -0.1920672 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize comments\n",
    "tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n",
    "\n",
    "# Train word embeddings\n",
    "model = Word2Vec(tokenized_comments, vector_size=4, window=2, min_count=1, workers=1)\n",
    "\n",
    "# View a sample word vector\n",
    "print(\"Vector for 'love':\", model.wv['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nd8gg9iFg7Hz",
    "outputId": "cb0ceb36-cb7e-49a7-aa32-f36abf79acb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vector for first comment: [-0.01083414 -0.0337788   0.03695674  0.03914206]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average word vectors for each comment\n",
    "average_vectors = []\n",
    "for comment in tokenized_comments:\n",
    "  comment_vector = np.zeros(model.vector_size)\n",
    "  for word in comment:\n",
    "    try:\n",
    "      comment_vector += model.wv[word]\n",
    "    except KeyError:\n",
    "      # Ignore words not in the vocabulary\n",
    "      pass\n",
    "  average_vectors.append(comment_vector / len(comment))\n",
    "\n",
    "# Display the average vector for the first comment\n",
    "print(\"Average vector for first comment:\", average_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOAokq0zDMKa"
   },
   "source": [
    "When we talk about a \"100-dimensional vector,\" we're referring to a list or array of 100 numbers, each representing a point in some dimensional space. A word vector in such a space encapsulates various aspects of the word's meaning and usage.\n",
    "\n",
    "Understanding Dimensions and Averaging\n",
    "Let's say we have 3 words, each represented by a 4-dimensional word vector (for simplicity, we're using 4 dimensions instead of 100):\n",
    "\n",
    "- Word 1 vector: [1,2,3,4]\n",
    "- Word 2 vector: [2,3,4,5]\n",
    "- Word 3 vector: [3,4,5,6]\n",
    "  \n",
    "These vectors might be the embeddings for three words in a sentence. To represent the entire sentence by a single vector, we compute the average of these vectors.\n",
    "\n",
    "To find the average vector, we calculate the mean for each dimension across all word vectors:\n",
    "\n",
    "- Dimension 1 average: (1+2+3)/3=2\n",
    "- Dimension 2 average: (2+3+4)/3=3\n",
    "- Dimension 3 average: (3+4+5)/3=4\n",
    "- Dimension 4 average: (4+5+6)/3=5\n",
    "  \n",
    "So, the average vector representing the entire sentence is [2,3,4,5].\n",
    "\n",
    "What This Represents? This averaged vector is still in the same 4-dimensional space as the original word vectors, but it's a new vector that, in theory, captures the combined semantic and syntactic essence of all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/YigitAydede/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-08-25 15:27:18,544 : INFO : collecting all words and their counts\n",
      "2024-08-25 15:27:18,544 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-08-25 15:27:18,545 : INFO : collected 16 word types from a corpus of 20 raw words and 5 sentences\n",
      "2024-08-25 15:27:18,545 : INFO : Creating a fresh vocabulary\n",
      "2024-08-25 15:27:18,545 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 16 unique words (100.00% of original 16, drops 0)', 'datetime': '2024-08-25T15:27:18.545616', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:27:18,545 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 20 word corpus (100.00% of original 20, drops 0)', 'datetime': '2024-08-25T15:27:18.545843', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:27:18,546 : INFO : deleting the raw counts dictionary of 16 items\n",
      "2024-08-25 15:27:18,546 : INFO : sample=0.001 downsamples 16 most-common words\n",
      "2024-08-25 15:27:18,546 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2.7827416997969525 word corpus (13.9%% of prior 20)', 'datetime': '2024-08-25T15:27:18.546770', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-25 15:27:18,547 : INFO : estimated required memory for 16 words and 4 dimensions: 8512 bytes\n",
      "2024-08-25 15:27:18,547 : INFO : resetting layer weights\n",
      "2024-08-25 15:27:18,547 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-08-25T15:27:18.547697', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-08-25 15:27:18,547 : INFO : Word2Vec lifecycle event {'msg': 'training model with 1 workers on 16 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-08-25T15:27:18.547954', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-25 15:27:18,548 : INFO : EPOCH 0: training on 20 raw words (3 effective words) took 0.0s, 12876 effective words/s\n",
      "2024-08-25 15:27:18,549 : INFO : EPOCH 1: training on 20 raw words (4 effective words) took 0.0s, 18454 effective words/s\n",
      "2024-08-25 15:27:18,549 : INFO : EPOCH 2: training on 20 raw words (1 effective words) took 0.0s, 64691 effective words/s\n",
      "2024-08-25 15:27:18,550 : INFO : EPOCH 3: training on 20 raw words (2 effective words) took 0.0s, 126646 effective words/s\n",
      "2024-08-25 15:27:18,551 : INFO : EPOCH 4: training on 20 raw words (4 effective words) took 0.0s, 27883 effective words/s\n",
      "2024-08-25 15:27:18,551 : INFO : Word2Vec lifecycle event {'msg': 'training on 100 raw words (14 effective words) took 0.0s, 3976 effective words/s', 'datetime': '2024-08-25T15:27:18.551682', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-25 15:27:18,551 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16, vector_size=4, alpha=0.025>', 'datetime': '2024-08-25T15:27:18.551876', 'gensim': '4.3.2', 'python': '3.9.19 (main, May  6 2024, 14:39:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YigitAydede/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/YigitAydede/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/YigitAydede/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "\n",
    "# Download NLTK punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the dataset\n",
    "comments = [\"love the new features\", \"hate the long wait time\", \"excellent service\", \"poor experience with the product\", \"happy with the purchase\"]\n",
    "sentiments = [1, 0, 1, 0, 1]  # 1: Positive, 0: Negative\n",
    "\n",
    "# Tokenize comments\n",
    "tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n",
    "\n",
    "# Train Word2Vec embeddings\n",
    "model = Word2Vec(tokenized_comments, vector_size=4, window=2, min_count=1, workers=1)\n",
    "\n",
    "# Function to convert a comment to its average word vector\n",
    "def comment_to_vector(comment):\n",
    "    comment_vector = np.zeros(model.vector_size)\n",
    "    for word in comment:\n",
    "        if word in model.wv:\n",
    "            comment_vector += model.wv[word]\n",
    "    return comment_vector / len(comment)\n",
    "\n",
    "# Convert all tokenized comments to average word vectors\n",
    "average_vectors = [comment_to_vector(comment) for comment in tokenized_comments]\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(average_vectors, sentiments, test_size=0.4, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
