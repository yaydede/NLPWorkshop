{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP with Python - Basics**\n",
    "\n",
    "Dr. Aydede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating word embeddings from raw text involves several steps, including text preprocessing, tokenization, and the application of an embedding algorithm. Here's a detailed guide, along with the libraries commonly used in Python for each step:\n",
    "\n",
    "## 1. Text Preprocessing:\n",
    "\n",
    "Preprocessing is crucial to clean and normalize the text data. This step typically includes:\n",
    "\n",
    "- Lowercasing\n",
    "- Removing punctuation\n",
    "- Removing stop words\n",
    "- Lemmatization or stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yigitaydede/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yigitaydede/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yigitaydede/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yigitaydede/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens: ['sample', 'financial', 'report', 'text', 'number', 'punctuation', 'various', 'stop', 'word']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download NLTK data files if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Downloading the missing resource\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample text (replace with your financial report text)\n",
    "text = \"This is a sample financial report text with numbers, punctuations, and various stop words.\"\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Removing punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Removing stop words and lemmatization\n",
    "    return processed_tokens\n",
    "\n",
    "processed_tokens = preprocess_text(text)\n",
    "print(\"Processed Tokens:\", processed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines download necessary data files from the NLTK (Natural Language Toolkit) library:\n",
    "\n",
    "- `nltk.download('punkt')`: Downloads the Punkt tokenizer models, which are used for tokenizing text into sentences and words.\n",
    "- `nltk.download('stopwords')`: Downloads a list of common stop words in various languages. Stop words are words that are commonly filtered out in natural language processing tasks because they don't carry significant meaning (e.g., \"and\", \"the\", \"is\").\n",
    "- `nltk.download('wordnet')`: Downloads the WordNet lexical database, which is used for lemmatization, finding synonyms, and other lexical tasks.\n",
    "- `nltk.download('omw-1.4')`: Downloads the Open Multilingual WordNet package, which is needed for certain WordNet functions, particularly for handling multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines initialize the tools for lemmatization and stop word removal:\n",
    "\n",
    "- `WordNetLemmatizer()`: Creates an instance of the WordNet lemmatizer, which reduces words to their base or root form (e.g., \"running\" becomes \"run\").\n",
    "- `set(stopwords.words('english'))`: Creates a set of English stop words to efficiently check if a word is a stop word. Using a set makes membership tests faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuvUmUgE_IgE"
   },
   "source": [
    "## 2. `word2vec`\n",
    "\n",
    "Word embedding is a technique in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually, it involves the mathematical embedding from some space (e.g., the space of all possible words) to a lower-dimensional space of the real numbers. The key idea is to capture the semantic meanings, syntactic similarity, and relation of words in these vectors, such that words with similar meanings are closer to each other in the vector space.\n",
    "\n",
    "Word embedding models like Word2Vec, GloVe (Global Vectors for Word Representation), and FastText have become foundational in NLP applications because they can reduce the dimensionality of text data while preserving lexical and semantic word relationships.\n",
    "\n",
    "Let's take a simple example using Word2Vec from the Gensim library. Word2Vec can be trained with either the Continuous Bag of Words (CBOW) or Skip-Gram model. In CBOW, the model predicts a word given its context. In Skip-Gram, it predicts the context given a word. Here's how you can use Gensim to train a simple Word2Vec model on a small dataset:\n",
    "\n",
    "1. First, we'll create a small dataset (corpus).\n",
    "2. Then, we'll train a Word2Vec model on this dataset.\n",
    "3. Finally, we'll explore the resulting word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ik-e7US5cHyV",
    "outputId": "497b901c-b1ac-4aa0-f85a-f7e8f988bc5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 16:51:13,320 : INFO : collecting all words and their counts\n",
      "2024-08-05 16:51:13,322 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-08-05 16:51:13,323 : INFO : collected 18 word types from a corpus of 24 raw words and 4 sentences\n",
      "2024-08-05 16:51:13,324 : INFO : Creating a fresh vocabulary\n",
      "2024-08-05 16:51:13,325 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 18 unique words (100.0%% of original 18, drops 0)', 'datetime': '2024-08-05T16:51:13.325211', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-05 16:51:13,326 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 24 word corpus (100.0%% of original 24, drops 0)', 'datetime': '2024-08-05T16:51:13.326205', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-05 16:51:13,327 : INFO : deleting the raw counts dictionary of 18 items\n",
      "2024-08-05 16:51:13,328 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2024-08-05 16:51:13,329 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3.5679764105789453 word corpus (14.9%% of prior 24)', 'datetime': '2024-08-05T16:51:13.329002', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-05 16:51:13,330 : INFO : estimated required memory for 18 words and 100 dimensions: 23400 bytes\n",
      "2024-08-05 16:51:13,330 : INFO : resetting layer weights\n",
      "2024-08-05 16:51:13,331 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-08-05T16:51:13.331506', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-08-05 16:51:13,332 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 18 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-08-05T16:51:13.332172', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-05 16:51:13,334 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-08-05 16:51:13,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-08-05 16:51:13,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-08-05 16:51:13,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 16:51:13,337 : INFO : EPOCH - 1 : training on 24 raw words (4 effective words) took 0.0s, 1462 effective words/s\n",
      "2024-08-05 16:51:13,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-08-05 16:51:13,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-08-05 16:51:13,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-08-05 16:51:13,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 16:51:13,342 : INFO : EPOCH - 2 : training on 24 raw words (4 effective words) took 0.0s, 1479 effective words/s\n",
      "2024-08-05 16:51:13,344 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-08-05 16:51:13,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-08-05 16:51:13,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-08-05 16:51:13,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 16:51:13,347 : INFO : EPOCH - 3 : training on 24 raw words (3 effective words) took 0.0s, 1150 effective words/s\n",
      "2024-08-05 16:51:13,349 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-08-05 16:51:13,349 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-08-05 16:51:13,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-08-05 16:51:13,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 16:51:13,350 : INFO : EPOCH - 4 : training on 24 raw words (3 effective words) took 0.0s, 2037 effective words/s\n",
      "2024-08-05 16:51:13,353 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2024-08-05 16:51:13,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2024-08-05 16:51:13,354 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2024-08-05 16:51:13,355 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 16:51:13,355 : INFO : EPOCH - 5 : training on 24 raw words (3 effective words) took 0.0s, 1081 effective words/s\n",
      "2024-08-05 16:51:13,356 : INFO : Word2Vec lifecycle event {'msg': 'training on 120 raw words (17 effective words) took 0.0s, 716 effective words/s', 'datetime': '2024-08-05T16:51:13.356736', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-05 16:51:13,357 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=18, vector_size=100, alpha=0.025)', 'datetime': '2024-08-05T16:51:13.357563', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model: Word2Vec(vocab=18, vector_size=100, alpha=0.025)\n",
      "Vector for 'python': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "Words similar to 'python': [('learning', 0.21883945167064667), ('with', 0.21617388725280762), ('easy', 0.0931011214852333), ('and', 0.0929069072008133), ('programs', 0.07961685955524445), ('languages', 0.06285078078508377), ('machine', 0.05433366447687149), ('java', 0.02702956274151802), ('language', 0.016139546409249306), ('is', -0.01083618775010109)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "# Enable logging for monitoring training\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['python', 'is', 'a', 'programming', 'language'],\n",
    "    ['python', 'and', 'java', 'are', 'popular', 'programming', 'languages'],\n",
    "    ['python', 'programs', 'are', 'easy', 'to', 'write'],\n",
    "    ['machine', 'learning', 'is', 'fun', 'with', 'python']\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Summarize the loaded model\n",
    "print(\"Word2Vec model:\", model)\n",
    "\n",
    "# Access vectors for one word\n",
    "print(\"Vector for 'python':\", model.wv['python'])\n",
    "\n",
    "# Find most similar words\n",
    "print(\"Words similar to 'python':\", model.wv.most_similar('python'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the basics of training a Word2Vec model with Gensim. Here, `vector_size` specifies the dimensionality of the word vectors, `window` defines the maximum distance between the current and predicted word within a sentence, and `min_count` ignores all words with total frequency lower than this.\n",
    "\n",
    "After training, we access the vector for \"python\" and find words similar to \"python\" based on their word embeddings. The output will give you an insight into how the model understands \"python\" in the context of the provided corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings - Details\n",
    "\n",
    "### Popular Word Embedding Models\n",
    "These are some of the most popular and widely used word embedding models: `Word2Vec`, `GloVe`, and `FastText`.\n",
    "\n",
    "`Word2Vec`, `GloVe`, and `FastText` are different in their approaches to generating word embeddings, though they share the common goal of representing words as vectors in a continuous vector space. Here’s a brief comparison:\n",
    "\n",
    "1. `Word2Vec`\n",
    "- Developed by: Google.\n",
    "- Approach: Predictive model.\n",
    "- Architecture: Uses neural networks with either Continuous Bag of Words (CBOW) or Skip-gram models.\n",
    "    - CBOW: Predicts a target word from a window of context words.\n",
    "    - Skip-gram: Predicts context words from a target word.\n",
    "- Training: Trained on large corpora of text, learning to predict words given their context.\n",
    "- Output: Dense vector representations for each word.\n",
    "\n",
    "2. `GloVe`\n",
    "- Developed by: Stanford.\n",
    "- Approach: Count-based model.\n",
    "- Architecture: Constructs a co-occurrence matrix of words from a corpus, then factorizes this matrix to find word vectors.\n",
    "- Training: Uses the statistical information contained in a corpus, specifically the co-occurrence matrix, to find vector representations that capture the probability of word co-occurrences.\n",
    "- Output: Dense vector representations for each word.\n",
    "\n",
    "3. `FastText`\n",
    "- Developed by: Facebook.\n",
    "- Approach: Predictive model with subword information.\n",
    "- Architecture: Similar to Word2Vec (CBOW or Skip-gram), but includes subword (character n-grams) information.\n",
    "- Training: Trains on large corpora of text, learning to predict words given their context, while incorporating subword information.\n",
    "- Output: Dense vector representations for each word, incorporating subword information.\n",
    "\n",
    "Each method has its own strengths and weaknesses, and the choice between them depends on the specific requirements of the task at hand, such as the size of the training data, the importance of handling out-of-vocabulary words, and the computational resources available.\n",
    "\n",
    "### Word2Vec - CBOW & Skip-gram\n",
    "Word2Vec is a popular technique for generating word embeddings, which are dense vector representations of words in a continuous vector space. There are two main approaches within Word2Vec: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "\n",
    "#### Continuous Bag of Words (CBOW)\n",
    "CBOW predicts the current word based on the context words surrounding it. It uses a sliding window of fixed size to capture the context around the target word. The model learns to predict the target word given the context words.\n",
    "\n",
    "How CBOW works:\n",
    "1. Context Window: Select a context window size (e.g., 2 words on either side of the target word).\n",
    "2. Context Words: For a given target word, identify the words within the context window.\n",
    "3. Prediction: Use the context words as input to predict the target word.\n",
    "Learning: The neural network adjusts its weights to minimize the prediction error.\n",
    "\n",
    "Example:\n",
    "- Sentence: \"The quick brown fox jumps over the lazy dog\"\n",
    "- Context window size = 2\n",
    "- Target word = \"brown\"\n",
    "- Context words = [\"The\", \"quick\", \"fox\", \"jumps\"]\n",
    "\n",
    "The CBOW model would take the context words (\"The\", \"quick\", \"fox\", \"jumps\") as input and try to predict the target word \"brown\".\n",
    "\n",
    "#### Skip-gram\n",
    "Skip-gram, on the other hand, predicts the context words based on the current word. It uses the current word as the input and predicts the surrounding context words.\n",
    "\n",
    "How it works:\n",
    "1. Context Window: Select a context window size (e.g., 2 words on either side of the target word).\n",
    "2. Target Word: For a given context window, identify the target word.\n",
    "3. Prediction: Use the target word as input to predict the context words.\n",
    "Learning: The neural network adjusts its weights to minimize the prediction error.\n",
    "\n",
    "Example:\n",
    "- Sentence: \"The quick brown fox jumps over the lazy dog\"\n",
    "- Context window size = 2\n",
    "- Target word = \"brown\"\n",
    "- Context words = [\"The\", \"quick\", \"fox\", \"jumps\"]\n",
    "\n",
    "The Skip-gram model would take the target word \"brown\" as input and try to predict the context words (\"The\", \"quick\", \"fox\", \"jumps\").\n",
    "\n",
    "#### Comparison\n",
    "- CBOW: Faster to train, works well with smaller datasets, and averages the context, which can smooth out noise.\n",
    "- Skip-gram: Slower to train but can produce higher-quality embeddings, especially for infrequent words, as it focuses on each context-target pair individually.\n",
    "\n",
    "Both architectures aim to create word embeddings that capture semantic relationships between words based on their contexts, but they do so in different ways. CBOW predicts the target word from its context, while Skip-gram predicts the context from the target word.\n",
    "\n",
    "### Data structure\n",
    "\n",
    "To understand how the data looks in CBOW (Continuous Bag of Words) using `Word2Vec` with one-hot encoding, let's walk through an example step-by-step.  Suppose we have 5 sentences each has different numbers of words\n",
    "\n",
    "When dealing with multiple sentences of different lengths in the context of training `Word2Vec` using the CBOW model, the process involves creating context-target pairs for each sentence individually. Here's how it works:\n",
    "\n",
    "General Approach\n",
    "1. Tokenization: Each sentence is broken down into individual words.\n",
    "2. Vocabulary Creation: A vocabulary of unique words across all sentences is created. Each word is assigned a unique index.\n",
    "3. One-Hot Encoding: Each word in the vocabulary is represented as a one-hot encoded vector.\n",
    "\n",
    "The CBOW model processes each sentence independently, generating context-target pairs based on the chosen context window size. Let's illustrate this with an example:\n",
    "\n",
    "Example Sentences\n",
    "1. \"I love natural language processing.\"\n",
    "2. \"Word2Vec is a popular algorithm.\"\n",
    "3. \"CBOW and Skip-gram are two models.\"\n",
    "4. \"Training word embeddings is important.\"\n",
    "5. \"Handling different sentence lengths.\"\n",
    "\n",
    "First, we tokenize each sentence:\n",
    "\n",
    "1. [\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n",
    "2. [\"Word2Vec\", \"is\", \"a\", \"popular\", \"algorithm\"]\n",
    "3. [\"CBOW\", \"and\", \"Skip-gram\", \"are\", \"two\", \"models\"]\n",
    "4. [\"Training\", \"word\", \"embeddings\", \"is\", \"important\"]\n",
    "5. [\"Handling\", \"different\", \"sentence\", \"lengths\"]\n",
    "\n",
    "Then, we create a vocabulary (Assume each word is assigned an index based on its order in the vocabulary):\n",
    "\"I\", \"love\", \"natural\", \"language\", \"processing\", \"Word2Vec\", \"is\", \"a\", \"popular\", \"algorithm\", \"CBOW\", \"and\", \"Skip-gram\", \"are\", \"two\", \"models\", \"Training\", \"word\", \"embeddings\", \"important\", \"Handling\", \"different\", \"sentence\", \"lengths\"\n",
    "\n",
    "#### Context-Target Pairs\n",
    "\n",
    "Now, let's create context-target pairs for each sentence:\n",
    "1. Context-Target Pair Generation\n",
    "For each sentence, context-target pairs are generated based on the chosen context window size. Let's assume the context window size is 2.\n",
    "\n",
    "Sentence 1: \"I love natural language processing.\"\n",
    "1. Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "2. Target: \"language\" | Context: [\"love\", \"natural\"]\n",
    "3. Target: \"processing\" | Context: [\"natural\", \"language\"]\n",
    "Sentence 2: \"Word2Vec is a popular algorithm.\"\n",
    "1. Target: \"is\" | Context: [\"Word2Vec\"]\n",
    "2. Target: \"a\" | Context: [\"Word2Vec\", \"is\"]\n",
    "3. Target: \"popular\" | Context: [\"is\", \"a\"]\n",
    "4. Target: \"algorithm\" | Context: [\"a\", \"popular\"]\n",
    "\n",
    "And so on.  In the case of sentences with different lengths, the process remains the same. The context-target pairs are generated for each sentence independently. The context window size is applied to each sentence, and the target word is selected from the context window.\n",
    "\n",
    "#### Understanding the Input Matrix for CBOW\n",
    "\n",
    "1. Context Window: For each target word, we consider a window of context words around it. Let's assume a context window size of 2 for this explanation.\n",
    "2. Context-Target Pairs: Each pair consists of context words as input and a target word as output.\n",
    "3. One-Hot Encoding: Each word in the context and target is represented as a one-hot encoded vector of length equal to the vocabulary size (24 in this case).\n",
    "\n",
    "Example Breakdown\n",
    "Given the five sentences and a vocabulary of 24 unique words, let's outline the context-target pairs and how they form the input matrix.\n",
    "\n",
    "Generating Context-Target Pairs for the first 2 sentences:\n",
    "Sentence 1: \"I love natural language processing.\"\n",
    "Pairs:\n",
    "1. Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "2. Target: \"language\" | Context: [\"love\", \"natural\"]\n",
    "3. Target: \"processing\" | Context: [\"natural\", \"language\"]\n",
    "Sentence 2: \"Word2Vec is a popular algorithm.\"\n",
    "Pairs:\n",
    "1. Target: \"is\" | Context: [\"Word2Vec\"]\n",
    "2. Target: \"a\" | Context: [\"Word2Vec\", \"is\"]\n",
    "3. Target: \"popular\" | Context: [\"is\", \"a\"]\n",
    "4. Target: \"algorithm\" | Context: [\"a\", \"popular\"]\n",
    "\n",
    "Context Matrix Structure\n",
    "For each context-target pair, the context words are one-hot encoded, and these one-hot encoded vectors are concatenated to form the input matrix.\n",
    "\n",
    "Example Pair\n",
    "Pair: Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "\n",
    "One-hot Encoding:  \n",
    "\"I\": [1, 0, 0, 0, ..., 0]\n",
    "\"love\": [0, 1, 0, 0, ..., 0]\n",
    "\n",
    "If we consider each context-target pair as an individual training example, the input (context) matrix for all pairs combined would be structured with each row representing a context word vector and columns representing features (words in the vocabulary).\n",
    "\n",
    "Example Scenario\n",
    "Given the same sentence: \"I love natural language processing.\"\n",
    "\n",
    "Vocabulary size (V) = 10\n",
    "Context window size = 2\n",
    "One-Hot Encoding Representation\n",
    "Assume the indices for one-hot encoding are:\n",
    "\n",
    "I -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "love -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "natural -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "language -> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "processing -> [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "Word2Vec -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "is -> [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "a -> [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "popular -> [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "algorithm -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "Context-Target Pairs for \"I love natural language processing.\"\n",
    "For context window size of 2 :\n",
    "1. Target: \"natural\" | Context: [\"I\", \"love\"]\n",
    "- Input (context): Sum of $[1,0,0,0,0,0,0,0,0,0]$ and $[0,1,0,0,0,0,0,0,0,0]$\n",
    "- Target: $[0,0,1,0,0,0,0,0,0,0]$\n",
    "2. Target: \"language\" | Context: [\"love\", \"natural\"]\n",
    "- Input (context): Sum of $[0,1,0,0,0,0,0,0,0,0]$ and $[0,0,1,0,0,0,0,0,0,0]$\n",
    "- Target: $[0,0,0,1,0,0,0,0,0,0]$\n",
    "3. Target: \"processing\" | Context: [\"natural\", \"language\"]\n",
    "- Input (context): Sum of $[0,0,1,0,0,0,0,0,0,0]$ and $[0,0,0,1,0,0,0,0,0,0]$\n",
    "- Target: $[0,0,0,0,1,0,0,0,0,0]$\n",
    "\n",
    "Input and Target Matrices\n",
    "Input Matrix (Summed Context Vectors):\n",
    "$$\n",
    "\\left[\\begin{array}{llllllllll}\n",
    "1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Target Matrix:\n",
    "$$\n",
    "\\left[\\begin{array}{llllllllll}\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "By summing the one-hot encoded vectors of the context words, you create a combined context representation that keeps the 1s from each context word. This approach simplifies the input matrix and ensures the dimensions are aligned correctly:\n",
    "\n",
    "Input Matrix Dimensions: Number of context-target pairs (3) × Vocabulary size (10)\n",
    "Target Matrix Dimensions: Number of context-target pairs (3) × Vocabulary size (10)\n",
    "This ensures that each row in the input matrix corresponds to a combined context vector, and each row in the target matrix corresponds to the target word's one-hot encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zviEL7BAh92"
   },
   "source": [
    "\n",
    "## **It's single layer NN with 100 nodes**\n",
    "  \n",
    "Let's clarify a bit more about what happens inside Word2Vec.\n",
    "\n",
    "Word2Vec, whether using the Continuous Bag of Words (CBOW) or Skip-Gram model, indeed leverages a neural network architecture, but it's structured a bit differently than a typical feedforward neural network with a single layer of 100 nodes (when you set vector_size=100). The \"100 nodes\" or \"100 dimensions\" represent the size of the word vectors you're aiming to learn, not the nodes of a hidden layer in a traditional sense.\n",
    "\n",
    "Here's a simplified overview of the process for both CBOW and Skip-Gram models:\n",
    "\n",
    "Input Layer: For CBOW, the input is the context words (multiple words), which are one-hot encoded vectors representing the presence of words in the context. For Skip-Gram, the input is just the target word. The size of each input vector is equal to the vocabulary size.\n",
    "\n",
    "Projection Layer (or Hidden Layer): This is not a typical hidden layer with activation functions. Instead, it's a projection layer where the actual learning of word embeddings happens. When you set vector_size=100, it means this layer will have 100 neurons. The weights connecting the input layer to this layer are what become the word embeddings. In training, for a given input word, the corresponding row in the weight matrix is essentially the word vector for that word.\n",
    "\n",
    "In CBOW, the vectors from the projection layer corresponding to each context word are averaged before being passed to the output layer.\n",
    "In Skip-Gram, the projection layer directly connects to the output layer, using the vector of the input word.\n",
    "Output Layer: The output layer is a softmax layer that makes predictions. For CBOW, it predicts the target word from the context. For Skip-Gram, it predicts the context words from the target word. The size of this layer is also equal to the vocabulary size.\n",
    "\n",
    "So, in summary:\n",
    "\n",
    "The \"100 dimensions\" are essentially the weights of the projection layer that you learn during training.\n",
    "  \n",
    "The learning involves adjusting these weights so that the model gets better at its prediction task (predicting context words for Skip-Gram, predicting a target word for CBOW), thereby capturing semantic and syntactic word relationships in the process.\n",
    "The neural network aspect of Word2Vec is quite specialized and optimized for the task of learning word embeddings, which is a bit different from a general-purpose neural network used for other types of prediction tasks.\n",
    "\n",
    "In the context of the Word2Vec architecture and specifically regarding the projection (or hidden) layer where the word embeddings are learned, the activation function can indeed be thought of as an identity function, $f(x)=x$. This means that the output of each neuron in this layer is the same as its input, without any nonlinear transformation applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "787vic-hCBnC"
   },
   "source": [
    "## 2. Sentiment with labels\n",
    "\n",
    "Let's have a simple example\n",
    "\n",
    "After having a \"toy\" dataset, we tokenize the data.  Tokenization is a fundamental step in natural language processing (NLP) and plays a crucial role in preparing text data for training word embeddings or any other machine learning model. Here's what it does and why it's important in the context of training word embeddings, like in the sentiment analysis example:\n",
    "\n",
    "What Tokenization Does:\n",
    "Splits Text into Tokens: Tokenization breaks down text into its basic units (tokens), which are typically words or subwords. For instance, the sentence \"I love machine learning\" would be tokenized into [\"I\", \"love\", \"machine\", \"learning\"].\n",
    "  \n",
    "Facilitates Vector Representation: Each token (word) can then be represented as a vector in the word embedding space. This is crucial for training embeddings, as the algorithm needs to work with individual words to learn their semantic and syntactic relationships.\n",
    "  \n",
    "Removes Punctuation and Special Characters: Depending on the tokenizer, it can also help clean the text by removing punctuation, special characters, or unnecessary whitespace, making the text more uniform and easier to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOXS5MrTdf8G",
    "outputId": "5ad31e42-d235-485c-803b-05bc57c7221d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yigitaydede/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "comments = [\"love the new features\", \"hate the long wait time\", \"excellent service\", \"poor experience with the product\", \"happy with the purchase\"]\n",
    "sentiments = [1, 0, 1, 0, 1]  # 1: Positive, 0: Negative\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Now you can proceed with tokenizing your text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69oCTJ0fgtS9",
    "outputId": "8fe50d22-35f1-43d8-f6a9-dac1ad1f5d69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:04:44,619 : INFO : collecting all words and their counts\n",
      "2024-08-05 17:04:44,620 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-08-05 17:04:44,621 : INFO : collected 16 word types from a corpus of 20 raw words and 5 sentences\n",
      "2024-08-05 17:04:44,621 : INFO : Creating a fresh vocabulary\n",
      "2024-08-05 17:04:44,622 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 16 unique words (100.0%% of original 16, drops 0)', 'datetime': '2024-08-05T17:04:44.622604', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-05 17:04:44,624 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 20 word corpus (100.0%% of original 20, drops 0)', 'datetime': '2024-08-05T17:04:44.624632', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-05 17:04:44,625 : INFO : deleting the raw counts dictionary of 16 items\n",
      "2024-08-05 17:04:44,626 : INFO : sample=0.001 downsamples 16 most-common words\n",
      "2024-08-05 17:04:44,627 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2.7827416997969525 word corpus (13.9%% of prior 20)', 'datetime': '2024-08-05T17:04:44.627203', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-08-05 17:04:44,628 : INFO : estimated required memory for 16 words and 4 dimensions: 8512 bytes\n",
      "2024-08-05 17:04:44,628 : INFO : resetting layer weights\n",
      "2024-08-05 17:04:44,629 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-08-05T17:04:44.629761', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-08-05 17:04:44,631 : INFO : Word2Vec lifecycle event {'msg': 'training model with 1 workers on 16 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-08-05T17:04:44.631042', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-05 17:04:44,633 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 17:04:44,633 : INFO : EPOCH - 1 : training on 20 raw words (3 effective words) took 0.0s, 4577 effective words/s\n",
      "2024-08-05 17:04:44,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 17:04:44,635 : INFO : EPOCH - 2 : training on 20 raw words (4 effective words) took 0.0s, 3076 effective words/s\n",
      "2024-08-05 17:04:44,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 17:04:44,639 : INFO : EPOCH - 3 : training on 20 raw words (1 effective words) took 0.0s, 1003 effective words/s\n",
      "2024-08-05 17:04:44,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 17:04:44,642 : INFO : EPOCH - 4 : training on 20 raw words (2 effective words) took 0.0s, 1587 effective words/s\n",
      "2024-08-05 17:04:44,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2024-08-05 17:04:44,646 : INFO : EPOCH - 5 : training on 20 raw words (4 effective words) took 0.0s, 3789 effective words/s\n",
      "2024-08-05 17:04:44,646 : INFO : Word2Vec lifecycle event {'msg': 'training on 100 raw words (14 effective words) took 0.0s, 926 effective words/s', 'datetime': '2024-08-05T17:04:44.646946', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-08-05 17:04:44,647 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=16, vector_size=4, alpha=0.025)', 'datetime': '2024-08-05T17:04:44.647219', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-14.6-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'love': [-0.03944132  0.00803429 -0.10351574 -0.1920672 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize comments\n",
    "tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n",
    "\n",
    "# Train word embeddings\n",
    "model = Word2Vec(tokenized_comments, vector_size=4, window=2, min_count=1, workers=1)\n",
    "\n",
    "# View a sample word vector\n",
    "print(\"Vector for 'love':\", model.wv['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nd8gg9iFg7Hz",
    "outputId": "cb0ceb36-cb7e-49a7-aa32-f36abf79acb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average vector for first comment: [-0.01083414 -0.0337788   0.03695674  0.03914206]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average word vectors for each comment\n",
    "average_vectors = []\n",
    "for comment in tokenized_comments:\n",
    "  comment_vector = np.zeros(model.vector_size)\n",
    "  for word in comment:\n",
    "    try:\n",
    "      comment_vector += model.wv[word]\n",
    "    except KeyError:\n",
    "      # Ignore words not in the vocabulary\n",
    "      pass\n",
    "  average_vectors.append(comment_vector / len(comment))\n",
    "\n",
    "# Display the average vector for the first comment\n",
    "print(\"Average vector for first comment:\", average_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOAokq0zDMKa"
   },
   "source": [
    "When we talk about a \"100-dimensional vector,\" we're referring to a list or array of 100 numbers, each representing a point in some dimensional space. A word vector in such a space encapsulates various aspects of the word's meaning and usage.\n",
    "\n",
    "Understanding Dimensions and Averaging\n",
    "Let's say we have 3 words, each represented by a 4-dimensional word vector (for simplicity, we're using 4 dimensions instead of 100):\n",
    "\n",
    "- Word 1 vector: [1,2,3,4]\n",
    "- Word 2 vector: [2,3,4,5]\n",
    "- Word 3 vector: [3,4,5,6]\n",
    "  \n",
    "These vectors might be the embeddings for three words in a sentence. To represent the entire sentence by a single vector, we compute the average of these vectors.\n",
    "\n",
    "To find the average vector, we calculate the mean for each dimension across all word vectors:\n",
    "\n",
    "- Dimension 1 average: (1+2+3)/3=2\n",
    "- Dimension 2 average: (2+3+4)/3=3\n",
    "- Dimension 3 average: (3+4+5)/3=4\n",
    "- Dimension 4 average: (4+5+6)/3=5\n",
    "  \n",
    "So, the average vector representing the entire sentence is [2,3,4,5].\n",
    "\n",
    "What This Represents? This averaged vector is still in the same 4-dimensional space as the original word vectors, but it's a new vector that, in theory, captures the combined semantic and syntactic essence of all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjUT90KLjAuS",
    "outputId": "4f903acc-d484-4e3c-b2c9-93bc9422c47f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate model on test data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 15\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(y_test, y_pred)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy of Logistic Regression model:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "# prompt: use logistic regression on test/train data\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(average_vectors, sentiments, test_size=0.2)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model on test data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Logistic Regression model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLzVha9EjggO"
   },
   "source": [
    "\n",
    "## Word Embedding\n",
    "\n",
    "The dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qd-_sA95jnDE",
    "outputId": "1c53ae5b-2a93-44ae-fde6-f662db067495"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz\"\n",
    "\n",
    "# Send a HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Make sure the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the response content as a gzip file\n",
    "    with gzip.open(BytesIO(response.content), 'rt') as read_file:\n",
    "        # Read the dataset into a pandas DataFrame\n",
    "        data = pd.read_json(read_file, lines=True)\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(data.head())\n",
    "else:\n",
    "    print(\"Failed to download the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uDjdZoskUjF",
    "outputId": "4b58911c-0fc2-457c-fb99-1fbfb9a97507"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhBaadoskZ9e"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "review_text = data.reviewText.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAvaE57UlQMF",
    "outputId": "0893e727-894f-4031-8e4a-2d5b4e82b1f6"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqYDsXOtlvEL",
    "outputId": "fd0609f7-5436-4ac4-d22e-136a4d597313"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming 'review_text' is a pandas Series where each entry is a list of tokens (words)\n",
    "# Convert review_text into a list of lists of tokens for training\n",
    "sentences = review_text.tolist()\n",
    "\n",
    "# Initialize and train the Word2Vec model\n",
    "model = Word2Vec(sentences=sentences,\n",
    "                 vector_size=100,  # Size of word vectors; adjust based on your needs\n",
    "                 window=10,\n",
    "                 min_count=2,\n",
    "                 workers=4)\n",
    "\n",
    "# Summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# Save the model for later use\n",
    "model.save(\"word2vec_amazon_reviews.model\")\n",
    "\n",
    "# Access vectors for a word\n",
    "print(\"Vector for the word 'phone':\", model.wv['phone'])\n",
    "\n",
    "# Find most similar words to 'phone'\n",
    "print(\"Words similar to 'phone':\", model.wv.most_similar('phone'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-FzbuvGnRy-",
    "outputId": "49fa5c93-9f25-4e0f-94b1-38323555891d"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.build_vocab(review_text, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eGCRTbdnmvp",
    "outputId": "d2e78f3c-c353-4084-dfb2-c29e5730131e"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2aZHJs4oTfu"
   },
   "source": [
    "The first number (61502857): This is the total number of words processed during the training phase. It takes into account the window parameter and possibly multiple passes over the data, depending on the number of epochs the model is trained for. This number shows how many individual word contexts the training algorithm has used to adjust the vector representations.\n",
    "\n",
    "The second number (83868975): This is the total number of raw words in the training data. It represents the sum of the lengths of all the sentences provided to the model as training data, before any filtering for min_count (minimum word frequency) or other preprocessing steps. Essentially, it's the size of the training corpus in terms of total words before any words are excluded based on the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cj_JlFboWdN",
    "outputId": "492f334f-b384-4b0d-c55d-f7495f6d641f"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qclz2lM2odfm",
    "outputId": "13e3de4f-2832-40b2-ab40-e223c8fe4d45"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"great\", w2=\"great\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLWyFK9Zoorj",
    "outputId": "00922a61-0e1e-4f15-c2e7-992563154337"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"great\", w2=\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghJ6kqL6o8uZ"
   },
   "source": [
    "## Finding Similar Words\n",
    "\n",
    "After we've trained your Word2Vec model on customer reviews, we've essentially transformed words into vectors that capture semantic meanings, relationships, and context within our dataset. This opens up a variety of ways to analyze and gain insights from the customer reviews. Here are some practical applications and analyses we can perform:\n",
    "\n",
    "1. Finding Similar Words (we did already)\n",
    "Discover words that are semantically related to specific terms. This can help identify common themes or issues in reviews. For example, finding words similar to \"battery\" might reveal related concerns or praises in the context of product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "en_HhwNOpLTc",
    "outputId": "76a24130-8890-4410-dbe4-bcb2898735aa"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('battery', topn=10)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOr_y61EpRU6"
   },
   "source": [
    "## Word Clustering\n",
    "\n",
    "Cluster words based on their vector representations. This can help identify groups of related terms or concepts within the reviews. Techniques like K-means clustering can be applied to the word vectors to group words into clusters of similar meanings. Word clustering involves grouping words into clusters based on their vector representations, such that words in the same cluster have similar meanings or are used in similar contexts. This can reveal patterns, themes, or topics common in your data. For instance, in customer reviews, you might find clusters around product features, customer service, shipping issues, etc.   \n",
    "Let's demonstrate word clustering using K-means on the Word2Vec embeddings you've trained. We'll use a subset of the most frequent words to make the clusters more interpretable. Finally, we'll discuss the insights that can be gained from this analysis.\n",
    "  \n",
    "**Step 1: Preparing Word Vectors**\n",
    "First, extract a set of word vectors from your Word2Vec model. For demonstration, we'll use the 100 most frequent words (excluding very common but less informative words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4T_rlcjpVZB"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `model` is your Word2Vec model\n",
    "\n",
    "# Extract the list of words & their vectors\n",
    "word_vectors = model.wv.vectors\n",
    "words = list(model.wv.index_to_key)\n",
    "\n",
    "# For a more focused analysis, consider filtering words by frequency or excluding stop words\n",
    "# This example uses all words for simplicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcrP-Bstqnty"
   },
   "source": [
    "**Step 2: Clustering Words**\n",
    "Now, we'll use K-means clustering to group these words into clusters based on their vector similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGSz4FYlpraP",
    "outputId": "23f08aae-40d6-48b7-c235-56469f2b805d"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Number of clusters\n",
    "k = 10  # Example: 10 clusters. Adjust based on your analysis needs.\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(word_vectors)\n",
    "\n",
    "# Assign each word to a cluster\n",
    "word_cluster_labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNghLxWjp7c-"
   },
   "source": [
    "**Step 3: Examining the Clusters**\n",
    "After clustering, let's examine which words ended up in the same clusters. This will give us an idea of the themes or topics present in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BthSky44p94k",
    "outputId": "eddafd2a-4145-4976-a602-25108990aecb"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of word clusters\n",
    "word_clusters = {i: [] for i in range(k)}\n",
    "for word, cluster_label in zip(words, word_cluster_labels):\n",
    "    word_clusters[cluster_label].append(word)\n",
    "\n",
    "# Display words in each cluster\n",
    "for cluster, words in word_clusters.items():\n",
    "    print(f\"Cluster {cluster}: {words[:10]}\")  # Displaying first 10 words for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77WjYvcxq2IK"
   },
   "source": [
    "**Insights from Word Clustering**.\n",
    "\n",
    "Theme Identification: Each cluster represents a group of words that are contextually similar. By examining the words in each cluster, you can identify common themes or topics in the reviews. For example, a cluster containing words like \"battery\", \"charge\", and \"power\" might indicate discussions about battery life.\n",
    "\n",
    "Product Features and Issues: Clusters might reveal specific product features that customers talk about the most, as well as recurring issues or areas of dissatisfaction.\n",
    "\n",
    "Customer Sentiment: Although not a direct measure of sentiment, the clustering of certain words together can give clues about overall customer sentiment. Words with positive connotations clustering together separately from words with negative connotations could indicate polarized opinions about certain aspects of the product or service.\n",
    "\n",
    "Improving Product and Service: By identifying clusters related to customer service, shipping, product durability, etc., businesses can pinpoint areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1NJO8y2rqnP"
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Now let's try a sentiment analysis.  Performing sentiment analysis without pre-labeled data is a common challenge, but there are several approaches you can take to analyze sentiment in your customer reviews.\n",
    "\n",
    "**Lexicon-Based Approach**\n",
    "  \n",
    "This method relies on predefined lists of words associated with positive and negative sentiments. You can use libraries like TextBlob or VADER, which come with built-in sentiment lexicons and can provide sentiment scores based on the presence and combinations of positive and negative words in your text.\n",
    "\n",
    "Here is an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTK3Euc9soa0",
    "outputId": "5476fe99-a332-49ba-f877-f278963ab441"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Example review\n",
    "review = \"The phone has an amazing battery life but a disappointing camera.\"\n",
    "\n",
    "# Get sentiment polarity\n",
    "sentiment = TextBlob(review).sentiment.polarity\n",
    "print(f\"Sentiment polarity: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlt5IWyTs9G3"
   },
   "source": [
    "A positive polarity score indicates a positive sentiment, while a negative score indicates a negative sentiment. TextBlob can be a straightforward way to start with sentiment analysis without needing labeled data.\n",
    "\n",
    "This method relies on predefined sentiment scores for words to evaluate the overall sentiment of a piece of text. Two popular tools for this purpose are TextBlob and VADER (Valence Aware Dictionary and sEntiment Reasoner), both of which are well-suited for different types of text data. Here, I'll show you how to use both, and you can choose based on your preference and the nature of your dataset.\n",
    "\n",
    "TextBlob is straightforward and works well for general-purpose sentiment analysis, including on longer texts like reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJFWtuVguXlF"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Applying TextBlob sentiment analysis on the reviewText column\n",
    "data['sentiment_polarity'] = data['reviewText'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "data['sentiment_subjectivity'] = data['reviewText'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh-slp_hyICP"
   },
   "source": [
    "Now that you have sentiment scores, you can analyze them to gain insights into the overall sentiment of the reviews, such as:\n",
    "\n",
    "Overall Sentiment: Calculate the average sentiment polarity to get an idea of the overall sentiment towards the product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUY_n4PLyKra",
    "outputId": "3f6a717e-7bd0-4f84-907d-4d099099a1f2"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "average_sentiment = data['sentiment_polarity'].mean()\n",
    "print(f\"Average Sentiment Polarity: {average_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIRFrtHgyZbL"
   },
   "source": [
    "An average sentiment polarity of approximately 0.248 suggests that the overall sentiment in your dataset of reviews leans towards the positive side. This is a good starting point for understanding customer sentiment, but there are several ways you can delve deeper to gain more nuanced insights.  Now that we know the overall sentiment is somewhat positive, we might want to understand how sentiment varies across different aspects or features of the product, like its battery life, camera quality, or customer service. We can filter reviews mentioning specific features and calculate the average sentiment for reviews concerning each aspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBySE3Cy8sxM",
    "outputId": "30458908-763b-4e91-89b5-e09b8c293c9f"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "positive_reviews = data[data['sentiment_polarity'] > 0].shape[0]\n",
    "neutral_reviews = data[data['sentiment_polarity'] == 0].shape[0]\n",
    "negative_reviews = data[data['sentiment_polarity'] < 0].shape[0]\n",
    "\n",
    "print(f\"Positive Reviews: {positive_reviews}\")\n",
    "print(f\"Neutral Reviews: {neutral_reviews}\")\n",
    "print(f\"Negative Reviews: {negative_reviews}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoNWQ6008-TM",
    "outputId": "b03c2c58-133d-49d2-81b1-941f1d7af922"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "features = ['battery', 'camera', 'service']\n",
    "for feature in features:\n",
    "    feature_reviews = data[data['reviewText'].str.contains(feature, case=False)]\n",
    "    avg_sentiment = feature_reviews['sentiment_polarity'].mean()\n",
    "    print(f\"Average sentiment for {feature}: {avg_sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (noconda_env)",
   "language": "python",
   "name": "noconda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
