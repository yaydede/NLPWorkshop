{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP with Python: Tokenization, Word Embedding (word2vec), and Sentiment Analysis**\n",
        "\n",
        "Dr. Aydede\n",
        "  \n",
        "## 1. `word2vec`\n",
        "\n",
        "Word embedding is a technique in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually, it involves the mathematical embedding from some space (e.g., the space of all possible words) to a lower-dimensional space of the real numbers. The key idea is to capture the semantic meanings, syntactic similarity, and relation of words in these vectors, such that words with similar meanings are closer to each other in the vector space.\n",
        "\n",
        "Word embedding models like Word2Vec, GloVe (Global Vectors for Word Representation), and FastText have become foundational in NLP applications because they can reduce the dimensionality of text data while preserving lexical and semantic word relationships.\n",
        "\n",
        "Let's take a simple example using Word2Vec from the Gensim library. Word2Vec can be trained with either the Continuous Bag of Words (CBOW) or Skip-Gram model. In CBOW, the model predicts a word given its context. In Skip-Gram, it predicts the context given a word. Here's how you can use Gensim to train a simple Word2Vec model on a small dataset:\n",
        "\n",
        "1. First, we'll create a small dataset (corpus).\n",
        "2. Then, we'll train a Word2Vec model on this dataset.\n",
        "3. Finally, we'll explore the resulting word embeddings."
      ],
      "metadata": {
        "id": "AuvUmUgE_IgE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik-e7US5cHyV",
        "outputId": "497b901c-b1ac-4aa0-f85a-f7e8f988bc5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model: Word2Vec<vocab=18, vector_size=100, alpha=0.025>\n",
            "Vector for 'python': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
            " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
            " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
            " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
            "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
            "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
            "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
            " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
            "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
            "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
            " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
            " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
            "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
            " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
            "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
            " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
            " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
            " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
            " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
            "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
            " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
            " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
            " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
            "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
            " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
            "Words similar to 'python': [('learning', 0.21883945167064667), ('with', 0.21617388725280762), ('easy', 0.0931011214852333), ('and', 0.09290692955255508), ('programs', 0.07961686700582504), ('languages', 0.06285078823566437), ('machine', 0.05433367192745209), ('java', 0.027029551565647125), ('language', 0.016139544546604156), ('is', -0.010836190544068813)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import logging\n",
        "\n",
        "# Enable logging for monitoring training\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    ['python', 'is', 'a', 'programming', 'language'],\n",
        "    ['python', 'and', 'java', 'are', 'popular', 'programming', 'languages'],\n",
        "    ['python', 'programs', 'are', 'easy', 'to', 'write'],\n",
        "    ['machine', 'learning', 'is', 'fun', 'with', 'python']\n",
        "]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Summarize the loaded model\n",
        "print(\"Word2Vec model:\", model)\n",
        "\n",
        "# Access vectors for one word\n",
        "print(\"Vector for 'python':\", model.wv['python'])\n",
        "\n",
        "# Find most similar words\n",
        "print(\"Words similar to 'python':\", model.wv.most_similar('python'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates the basics of training a Word2Vec model with Gensim. Here, `vector_size` specifies the dimensionality of the word vectors, `window` defines the maximum distance between the current and predicted word within a sentence, and `min_count` ignores all words with total frequency lower than this.\n",
        "\n",
        "After training, we access the vector for \"python\" and find words similar to \"python\" based on their word embeddings. The output will give you an insight into how the model understands \"python\" in the context of the provided corpus.\n",
        "\n",
        "**It's single layer NN with 100 nodes**\n",
        "  \n",
        "Let's clarify a bit more about what happens inside Word2Vec.\n",
        "\n",
        "Word2Vec, whether using the Continuous Bag of Words (CBOW) or Skip-Gram model, indeed leverages a neural network architecture, but it's structured a bit differently than a typical feedforward neural network with a single layer of 100 nodes (when you set vector_size=100). The \"100 nodes\" or \"100 dimensions\" represent the size of the word vectors you're aiming to learn, not the nodes of a hidden layer in a traditional sense.\n",
        "\n",
        "Here's a simplified overview of the process for both CBOW and Skip-Gram models:\n",
        "\n",
        "Input Layer: For CBOW, the input is the context words (multiple words), which are one-hot encoded vectors representing the presence of words in the context. For Skip-Gram, the input is just the target word. The size of each input vector is equal to the vocabulary size.\n",
        "\n",
        "Projection Layer (or Hidden Layer): This is not a typical hidden layer with activation functions. Instead, it's a projection layer where the actual learning of word embeddings happens. When you set vector_size=100, it means this layer will have 100 neurons. The weights connecting the input layer to this layer are what become the word embeddings. In training, for a given input word, the corresponding row in the weight matrix is essentially the word vector for that word.\n",
        "\n",
        "In CBOW, the vectors from the projection layer corresponding to each context word are averaged before being passed to the output layer.\n",
        "In Skip-Gram, the projection layer directly connects to the output layer, using the vector of the input word.\n",
        "Output Layer: The output layer is a softmax layer that makes predictions. For CBOW, it predicts the target word from the context. For Skip-Gram, it predicts the context words from the target word. The size of this layer is also equal to the vocabulary size.\n",
        "\n",
        "So, in summary:\n",
        "\n",
        "The \"100 dimensions\" are essentially the weights of the projection layer that you learn during training.\n",
        "  \n",
        "The learning involves adjusting these weights so that the model gets better at its prediction task (predicting context words for Skip-Gram, predicting a target word for CBOW), thereby capturing semantic and syntactic word relationships in the process.\n",
        "The neural network aspect of Word2Vec is quite specialized and optimized for the task of learning word embeddings, which is a bit different from a general-purpose neural network used for other types of prediction tasks.\n",
        "\n",
        "In the context of the Word2Vec architecture and specifically regarding the projection (or hidden) layer where the word embeddings are learned, the activation function can indeed be thought of as an identity function, $f(x)=x$. This means that the output of each neuron in this layer is the same as its input, without any nonlinear transformation applied."
      ],
      "metadata": {
        "id": "_zviEL7BAh92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Sentiment with labels\n",
        "\n",
        "Let's have a simple example\n",
        "\n",
        "After having a \"toy\" dataset, we tokenize the data.  Tokenization is a fundamental step in natural language processing (NLP) and plays a crucial role in preparing text data for training word embeddings or any other machine learning model. Here's what it does and why it's important in the context of training word embeddings, like in the sentiment analysis example:\n",
        "\n",
        "What Tokenization Does:\n",
        "Splits Text into Tokens: Tokenization breaks down text into its basic units (tokens), which are typically words or subwords. For instance, the sentence \"I love machine learning\" would be tokenized into [\"I\", \"love\", \"machine\", \"learning\"].\n",
        "  \n",
        "Facilitates Vector Representation: Each token (word) can then be represented as a vector in the word embedding space. This is crucial for training embeddings, as the algorithm needs to work with individual words to learn their semantic and syntactic relationships.\n",
        "  \n",
        "Removes Punctuation and Special Characters: Depending on the tokenizer, it can also help clean the text by removing punctuation, special characters, or unnecessary whitespace, making the text more uniform and easier to process."
      ],
      "metadata": {
        "id": "787vic-hCBnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset\n",
        "comments = [\"love the new features\", \"hate the long wait time\", \"excellent service\", \"poor experience with the product\", \"happy with the purchase\"]\n",
        "sentiments = [1, 0, 1, 0, 1]  # 1: Positive, 0: Negative\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Now you can proceed with tokenizing your text\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOXS5MrTdf8G",
        "outputId": "5ad31e42-d235-485c-803b-05bc57c7221d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# Tokenize comments\n",
        "tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n",
        "\n",
        "# Train word embeddings\n",
        "model = Word2Vec(tokenized_comments, vector_size=4, window=2, min_count=1, workers=1)\n",
        "\n",
        "# View a sample word vector\n",
        "print(\"Vector for 'love':\", model.wv['love'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69oCTJ0fgtS9",
        "outputId": "8fe50d22-35f1-43d8-f6a9-dac1ad1f5d69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'love': [-0.03944132  0.00803429 -0.10351574 -0.1920672 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average word vectors for each comment\n",
        "average_vectors = []\n",
        "for comment in tokenized_comments:\n",
        "  comment_vector = np.zeros(model.vector_size)\n",
        "  for word in comment:\n",
        "    try:\n",
        "      comment_vector += model.wv[word]\n",
        "    except KeyError:\n",
        "      # Ignore words not in the vocabulary\n",
        "      pass\n",
        "  average_vectors.append(comment_vector / len(comment))\n",
        "\n",
        "# Display the average vector for the first comment\n",
        "print(\"Average vector for first comment:\", average_vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd8gg9iFg7Hz",
        "outputId": "cb0ceb36-cb7e-49a7-aa32-f36abf79acb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average vector for first comment: [-0.01083414 -0.0337788   0.03695674  0.03914206]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we talk about a \"100-dimensional vector,\" we're referring to a list or array of 100 numbers, each representing a point in some dimensional space. A word vector in such a space encapsulates various aspects of the word's meaning and usage.\n",
        "\n",
        "Understanding Dimensions and Averaging\n",
        "Let's say we have 3 words, each represented by a 4-dimensional word vector (for simplicity, we're using 4 dimensions instead of 100):\n",
        "\n",
        "- Word 1 vector: [1,2,3,4]\n",
        "- Word 2 vector: [2,3,4,5]\n",
        "- Word 3 vector: [3,4,5,6]\n",
        "  \n",
        "These vectors might be the embeddings for three words in a sentence. To represent the entire sentence by a single vector, we compute the average of these vectors.\n",
        "\n",
        "To find the average vector, we calculate the mean for each dimension across all word vectors:\n",
        "\n",
        "- Dimension 1 average: (1+2+3)/3=2\n",
        "- Dimension 2 average: (2+3+4)/3=3\n",
        "- Dimension 3 average: (3+4+5)/3=4\n",
        "- Dimension 4 average: (4+5+6)/3=5\n",
        "  \n",
        "So, the average vector representing the entire sentence is [2,3,4,5].\n",
        "\n",
        "What This Represents? This averaged vector is still in the same 4-dimensional space as the original word vectors, but it's a new vector that, in theory, captures the combined semantic and syntactic essence of all the words in the text."
      ],
      "metadata": {
        "id": "iOAokq0zDMKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: use logistic regression on test/train data\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(average_vectors, sentiments, test_size=0.2)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model on test data\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Logistic Regression model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjUT90KLjAuS",
        "outputId": "4f903acc-d484-4e3c-b2c9-93bc9422c47f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Word Embedding\n",
        "\n",
        "The dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas."
      ],
      "metadata": {
        "id": "DLzVha9EjggO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import gzip\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz\"\n",
        "\n",
        "# Send a HTTP request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Make sure the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Open the response content as a gzip file\n",
        "    with gzip.open(BytesIO(response.content), 'rt') as read_file:\n",
        "        # Read the dataset into a pandas DataFrame\n",
        "        data = pd.read_json(read_file, lines=True)\n",
        "    # Display the first few rows of the DataFrame\n",
        "    print(data.head())\n",
        "else:\n",
        "    print(\"Failed to download the dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd-_sA95jnDE",
        "outputId": "1c53ae5b-2a93-44ae-fde6-f662db067495"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       reviewerID        asin      reviewerName helpful  \\\n",
            "0  A30TL5EWN6DFXT  120401325X         christina  [0, 0]   \n",
            "1   ASY55RVNIL0UD  120401325X          emily l.  [0, 0]   \n",
            "2  A2TMXE2AFO7ONB  120401325X             Erica  [0, 0]   \n",
            "3   AWJ0WZQYMYFQ4  120401325X                JM  [4, 4]   \n",
            "4   ATX7CZYFXI1KW  120401325X  patrice m rogoza  [2, 3]   \n",
            "\n",
            "                                          reviewText  overall  \\\n",
            "0  They look good and stick good! I just don't li...        4   \n",
            "1  These stickers work like the review says they ...        5   \n",
            "2  These are awesome and make my phone look so st...        5   \n",
            "3  Item arrived in great time and was in perfect ...        4   \n",
            "4  awesome! stays on, and looks great. can be use...        5   \n",
            "\n",
            "                                     summary  unixReviewTime   reviewTime  \n",
            "0                                 Looks Good      1400630400  05 21, 2014  \n",
            "1                      Really great product.      1389657600  01 14, 2014  \n",
            "2                             LOVE LOVE LOVE      1403740800  06 26, 2014  \n",
            "3                                      Cute!      1382313600  10 21, 2013  \n",
            "4  leopard home button sticker for iphone 4s      1359849600   02 3, 2013  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uDjdZoskUjF",
        "outputId": "4b58911c-0fc2-457c-fb99-1fbfb9a97507"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(194439, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_text = data.reviewText.apply(gensim.utils.simple_preprocess)"
      ],
      "metadata": {
        "id": "dhBaadoskZ9e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAvaE57UlQMF",
        "outputId": "0893e727-894f-4031-8e4a-2d5b4e82b1f6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         [they, look, good, and, stick, good, just, don...\n",
              "1         [these, stickers, work, like, the, review, say...\n",
              "2         [these, are, awesome, and, make, my, phone, lo...\n",
              "3         [item, arrived, in, great, time, and, was, in,...\n",
              "4         [awesome, stays, on, and, looks, great, can, b...\n",
              "                                ...                        \n",
              "194434    [works, great, just, like, my, original, one, ...\n",
              "194435    [great, product, great, packaging, high, quali...\n",
              "194436    [this, is, great, cable, just, as, good, as, t...\n",
              "194437    [really, like, it, becasue, it, works, well, w...\n",
              "194438    [product, as, described, have, wasted, lot, of...\n",
              "Name: reviewText, Length: 194439, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Assuming 'review_text' is a pandas Series where each entry is a list of tokens (words)\n",
        "# Convert review_text into a list of lists of tokens for training\n",
        "sentences = review_text.tolist()\n",
        "\n",
        "# Initialize and train the Word2Vec model\n",
        "model = Word2Vec(sentences=sentences,\n",
        "                 vector_size=100,  # Size of word vectors; adjust based on your needs\n",
        "                 window=10,\n",
        "                 min_count=2,\n",
        "                 workers=4)\n",
        "\n",
        "# Summarize the loaded model\n",
        "print(model)\n",
        "\n",
        "# Save the model for later use\n",
        "model.save(\"word2vec_amazon_reviews.model\")\n",
        "\n",
        "# Access vectors for a word\n",
        "print(\"Vector for the word 'phone':\", model.wv['phone'])\n",
        "\n",
        "# Find most similar words to 'phone'\n",
        "print(\"Words similar to 'phone':\", model.wv.most_similar('phone'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqYDsXOtlvEL",
        "outputId": "fd0609f7-5436-4ac4-d22e-136a4d597313"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=35561, vector_size=100, alpha=0.025>\n",
            "Vector for the word 'phone': [-2.1982071e+00 -8.0649149e-01  1.9280401e+00 -1.8713187e-03\n",
            " -1.8930609e+00  5.9042013e-01  3.6916751e-01 -1.5701485e+00\n",
            "  2.1311574e+00  2.4163215e+00  1.0375583e+00 -3.4427629e+00\n",
            " -1.5282893e+00 -2.2158711e+00  1.2268982e+00 -3.2126966e+00\n",
            "  8.0580190e-02  2.0776742e+00  1.1629301e+00  1.3575493e+00\n",
            " -1.7898109e+00  2.7559390e+00  1.2408208e+00  2.5871902e+00\n",
            " -7.0643437e-01  1.4327291e+00  3.6206458e+00  3.1199279e+00\n",
            "  3.4209259e+00 -1.4895647e+00  3.4860287e+00 -2.8240194e+00\n",
            "  2.5911493e+00  2.9806135e+00  8.1010908e-01 -5.1091522e-01\n",
            "  1.1223867e+00 -2.3968844e+00  2.1699142e+00  2.1751547e+00\n",
            " -1.3381577e+00  2.1534038e+00 -1.7891235e+00 -4.1032502e-01\n",
            "  7.0157099e-01 -2.6939659e+00  1.6133605e+00  1.7733831e+00\n",
            "  1.1621673e+00  4.5868835e-01  3.1874602e+00 -2.4393067e+00\n",
            " -2.8102043e+00 -1.6228411e+00  3.3430862e-01  8.7354320e-01\n",
            " -3.4092398e+00  1.7223221e+00  2.0883474e+00  3.4938743e+00\n",
            " -3.1526453e+00  4.9981806e-01 -2.5362840e+00  8.6771047e-01\n",
            " -9.9021018e-01 -4.2685313e+00  1.6413347e+00 -1.1578802e+00\n",
            " -1.8715855e+00  3.8874483e-01  8.9944315e-01 -1.6331296e+00\n",
            "  4.2136452e-01 -1.6379930e+00 -1.6974050e+00  5.0312740e-01\n",
            " -3.3991697e-01 -1.2227387e+00  5.4319561e-01 -3.5581625e-01\n",
            "  2.1168263e+00  1.4790198e+00 -4.7492754e-01  4.3621964e+00\n",
            " -2.3890586e+00  1.8423806e-01 -2.7693257e-01  7.0378518e-01\n",
            "  1.8121134e+00 -2.1140209e-01  3.7473908e-01 -2.4435983e+00\n",
            " -1.0337236e+00 -1.1255926e+00 -2.9687459e+00 -2.8970888e-01\n",
            "  3.7270057e+00  1.3267978e-01  1.6510162e+00 -3.6344952e-03]\n",
            "Words similar to 'phone': [('iphone', 0.5723087191581726), ('cellphone', 0.5294259190559387), ('it', 0.5241072177886963), ('lap', 0.48090872168540955), ('gn', 0.4762304425239563), ('case', 0.46658217906951904), ('device', 0.45599162578582764), ('cheek', 0.4484564960002899), ('fone', 0.44173580408096313), ('hand', 0.4222155213356018)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(review_text, progress_per=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-FzbuvGnRy-",
        "outputId": "49fa5c93-9f25-4e0f-94b1-38323555891d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eGCRTbdnmvp",
        "outputId": "d2e78f3c-c353-4084-dfb2-c29e5730131e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61502857, 83868975)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first number (61502857): This is the total number of words processed during the training phase. It takes into account the window parameter and possibly multiple passes over the data, depending on the number of epochs the model is trained for. This number shows how many individual word contexts the training algorithm has used to adjust the vector representations.\n",
        "\n",
        "The second number (83868975): This is the total number of raw words in the training data. It represents the sum of the lengths of all the sentences provided to the model as training data, before any filtering for min_count (minimum word frequency) or other preprocessing steps. Essentially, it's the size of the training corpus in terms of total words before any words are excluded based on the model's parameters."
      ],
      "metadata": {
        "id": "V2aZHJs4oTfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"bad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cj_JlFboWdN",
        "outputId": "492f334f-b384-4b0d-c55d-f7495f6d641f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('shabby', 0.6625680923461914),\n",
              " ('terrible', 0.6611157059669495),\n",
              " ('good', 0.5967464447021484),\n",
              " ('keen', 0.5830436944961548),\n",
              " ('horrible', 0.5354795455932617),\n",
              " ('okay', 0.5237018465995789),\n",
              " ('ok', 0.5104485154151917),\n",
              " ('cheap', 0.5079092383384705),\n",
              " ('poor', 0.49226653575897217),\n",
              " ('fault', 0.4708757996559143)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similarity(w1=\"great\", w2=\"great\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qclz2lM2odfm",
        "outputId": "13e3de4f-2832-40b2-ab40-e223c8fe4d45"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similarity(w1=\"great\", w2=\"good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLWyFK9Zoorj",
        "outputId": "00922a61-0e1e-4f15-c2e7-992563154337"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.78947496"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Similar Words\n",
        "\n",
        "After we've trained your Word2Vec model on customer reviews, we've essentially transformed words into vectors that capture semantic meanings, relationships, and context within our dataset. This opens up a variety of ways to analyze and gain insights from the customer reviews. Here are some practical applications and analyses we can perform:\n",
        "\n",
        "1. Finding Similar Words (we did already)\n",
        "Discover words that are semantically related to specific terms. This can help identify common themes or issues in reviews. For example, finding words similar to \"battery\" might reveal related concerns or praises in the context of product reviews."
      ],
      "metadata": {
        "id": "ghJ6kqL6o8uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = model.wv.most_similar('battery', topn=10)\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en_HhwNOpLTc",
        "outputId": "76a24130-8890-4410-dbe4-bcb2898735aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('batter', 0.8460943102836609), ('batt', 0.7913365364074707), ('batteries', 0.6934639811515808), ('itorch', 0.6278401613235474), ('powerbank', 0.5674920678138733), ('powerplant', 0.551176130771637), ('juice', 0.5456146597862244), ('prolong', 0.5360251069068909), ('incredicharge', 0.5267910361289978), ('span', 0.5267504453659058)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Clustering\n",
        "\n",
        "Cluster words based on their vector representations. This can help identify groups of related terms or concepts within the reviews. Techniques like K-means clustering can be applied to the word vectors to group words into clusters of similar meanings. Word clustering involves grouping words into clusters based on their vector representations, such that words in the same cluster have similar meanings or are used in similar contexts. This can reveal patterns, themes, or topics common in your data. For instance, in customer reviews, you might find clusters around product features, customer service, shipping issues, etc.   \n",
        "Let's demonstrate word clustering using K-means on the Word2Vec embeddings you've trained. We'll use a subset of the most frequent words to make the clusters more interpretable. Finally, we'll discuss the insights that can be gained from this analysis.\n",
        "  \n",
        "**Step 1: Preparing Word Vectors**\n",
        "First, extract a set of word vectors from your Word2Vec model. For demonstration, we'll use the 100 most frequent words (excluding very common but less informative words)."
      ],
      "metadata": {
        "id": "iOr_y61EpRU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Assuming `model` is your Word2Vec model\n",
        "\n",
        "# Extract the list of words & their vectors\n",
        "word_vectors = model.wv.vectors\n",
        "words = list(model.wv.index_to_key)\n",
        "\n",
        "# For a more focused analysis, consider filtering words by frequency or excluding stop words\n",
        "# This example uses all words for simplicity\n"
      ],
      "metadata": {
        "id": "s4T_rlcjpVZB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Clustering Words**\n",
        "Now, we'll use K-means clustering to group these words into clusters based on their vector similarities."
      ],
      "metadata": {
        "id": "BcrP-Bstqnty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of clusters\n",
        "k = 10  # Example: 10 clusters. Adjust based on your analysis needs.\n",
        "\n",
        "# Perform KMeans clustering\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(word_vectors)\n",
        "\n",
        "# Assign each word to a cluster\n",
        "word_cluster_labels = kmeans.labels_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGSz4FYlpraP",
        "outputId": "23f08aae-40d6-48b7-c235-56469f2b805d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Examining the Clusters**\n",
        "After clustering, let's examine which words ended up in the same clusters. This will give us an idea of the themes or topics present in the reviews."
      ],
      "metadata": {
        "id": "vNghLxWjp7c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of word clusters\n",
        "word_clusters = {i: [] for i in range(k)}\n",
        "for word, cluster_label in zip(words, word_cluster_labels):\n",
        "    word_clusters[cluster_label].append(word)\n",
        "\n",
        "# Display words in each cluster\n",
        "for cluster, words in word_clusters.items():\n",
        "    print(f\"Cluster {cluster}: {words[:10]}\")  # Displaying first 10 words for brevity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BthSky44p94k",
        "outputId": "eddafd2a-4145-4976-a602-25108990aecb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: ['and', 'to', 'is', 'for', 'that', 'in', 'with', 'but', 'not', 'as']\n",
            "Cluster 1: ['this', 'was', 'one', 'they', 'had', 'me', 'product', 'these', 'them', 'what']\n",
            "Cluster 2: ['of', 'if', 'there', 'any', 'used', 'thing', 'problem', 'being', 'people', 'headphones']\n",
            "Cluster 3: ['case', 'screen', 'back', 'protector', 'cover', 'looks', 'protection', 'cases', 'plastic', 'look']\n",
            "Cluster 4: ['sound', 'bluetooth', 'headset', 'music', 'life', 'speaker', 'camera', 'call', 'android', 'set']\n",
            "Cluster 5: ['have', 'be', 'use', 'get', 'do', 'work', 'recommend', 'need', 'buy', 'go']\n",
            "Cluster 6: ['like', 'can', 'great', 'good', 'would', 'will', 'well', 'does', 'don', 'much']\n",
            "Cluster 7: ['my', 'or', 'when', 'time', 'while', 'using', 'car', 'day', 'times', 'having']\n",
            "Cluster 8: ['the', 'it', 'on', 'phone', 'you', 'your', 'out', 'off', 'little', 'fit']\n",
            "Cluster 9: ['battery', 'an', 'iphone', 'charge', 'charger', 'other', 'which', 'works', 'usb', 'charging']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights from Word Clustering**.\n",
        "\n",
        "Theme Identification: Each cluster represents a group of words that are contextually similar. By examining the words in each cluster, you can identify common themes or topics in the reviews. For example, a cluster containing words like \"battery\", \"charge\", and \"power\" might indicate discussions about battery life.\n",
        "\n",
        "Product Features and Issues: Clusters might reveal specific product features that customers talk about the most, as well as recurring issues or areas of dissatisfaction.\n",
        "\n",
        "Customer Sentiment: Although not a direct measure of sentiment, the clustering of certain words together can give clues about overall customer sentiment. Words with positive connotations clustering together separately from words with negative connotations could indicate polarized opinions about certain aspects of the product or service.\n",
        "\n",
        "Improving Product and Service: By identifying clusters related to customer service, shipping, product durability, etc., businesses can pinpoint areas for improvement."
      ],
      "metadata": {
        "id": "77WjYvcxq2IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "Now let's try a sentiment analysis.  Performing sentiment analysis without pre-labeled data is a common challenge, but there are several approaches you can take to analyze sentiment in your customer reviews.\n",
        "\n",
        "**Lexicon-Based Approach**\n",
        "  \n",
        "This method relies on predefined lists of words associated with positive and negative sentiments. You can use libraries like TextBlob or VADER, which come with built-in sentiment lexicons and can provide sentiment scores based on the presence and combinations of positive and negative words in your text.\n",
        "\n",
        "Here is an example:\n",
        "\n"
      ],
      "metadata": {
        "id": "c1NJO8y2rqnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Example review\n",
        "review = \"The phone has an amazing battery life but a disappointing camera.\"\n",
        "\n",
        "# Get sentiment polarity\n",
        "sentiment = TextBlob(review).sentiment.polarity\n",
        "print(f\"Sentiment polarity: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTK3Euc9soa0",
        "outputId": "5476fe99-a332-49ba-f877-f278963ab441"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment polarity: 5.551115123125783e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A positive polarity score indicates a positive sentiment, while a negative score indicates a negative sentiment. TextBlob can be a straightforward way to start with sentiment analysis without needing labeled data.\n",
        "\n",
        "This method relies on predefined sentiment scores for words to evaluate the overall sentiment of a piece of text. Two popular tools for this purpose are TextBlob and VADER (Valence Aware Dictionary and sEntiment Reasoner), both of which are well-suited for different types of text data. Here, I'll show you how to use both, and you can choose based on your preference and the nature of your dataset.\n",
        "\n",
        "TextBlob is straightforward and works well for general-purpose sentiment analysis, including on longer texts like reviews."
      ],
      "metadata": {
        "id": "dlt5IWyTs9G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying TextBlob sentiment analysis on the reviewText column\n",
        "data['sentiment_polarity'] = data['reviewText'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "data['sentiment_subjectivity'] = data['reviewText'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n"
      ],
      "metadata": {
        "id": "HJFWtuVguXlF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have sentiment scores, you can analyze them to gain insights into the overall sentiment of the reviews, such as:\n",
        "\n",
        "Overall Sentiment: Calculate the average sentiment polarity to get an idea of the overall sentiment towards the product.\n"
      ],
      "metadata": {
        "id": "Mh-slp_hyICP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_sentiment = data['sentiment_polarity'].mean()\n",
        "print(f\"Average Sentiment Polarity: {average_sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUY_n4PLyKra",
        "outputId": "3f6a717e-7bd0-4f84-907d-4d099099a1f2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sentiment Polarity: 0.24830300849739492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An average sentiment polarity of approximately 0.248 suggests that the overall sentiment in your dataset of reviews leans towards the positive side. This is a good starting point for understanding customer sentiment, but there are several ways you can delve deeper to gain more nuanced insights.  Now that we know the overall sentiment is somewhat positive, we might want to understand how sentiment varies across different aspects or features of the product, like its battery life, camera quality, or customer service. We can filter reviews mentioning specific features and calculate the average sentiment for reviews concerning each aspect:"
      ],
      "metadata": {
        "id": "oIRFrtHgyZbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_reviews = data[data['sentiment_polarity'] > 0].shape[0]\n",
        "neutral_reviews = data[data['sentiment_polarity'] == 0].shape[0]\n",
        "negative_reviews = data[data['sentiment_polarity'] < 0].shape[0]\n",
        "\n",
        "print(f\"Positive Reviews: {positive_reviews}\")\n",
        "print(f\"Neutral Reviews: {neutral_reviews}\")\n",
        "print(f\"Negative Reviews: {negative_reviews}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBySE3Cy8sxM",
        "outputId": "30458908-763b-4e91-89b5-e09b8c293c9f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Reviews: 172070\n",
            "Neutral Reviews: 4950\n",
            "Negative Reviews: 17419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['battery', 'camera', 'service']\n",
        "for feature in features:\n",
        "    feature_reviews = data[data['reviewText'].str.contains(feature, case=False)]\n",
        "    avg_sentiment = feature_reviews['sentiment_polarity'].mean()\n",
        "    print(f\"Average sentiment for {feature}: {avg_sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoNWQ6008-TM",
        "outputId": "b03c2c58-133d-49d2-81b1-941f1d7af922"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sentiment for battery: 0.199119172408153\n",
            "Average sentiment for camera: 0.19423316438082053\n",
            "Average sentiment for service: 0.22639224564186428\n"
          ]
        }
      ]
    }
  ]
}